The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) nvidia/.latest   2) slurm/.latest
-------------------------------------------------------------------------------
There are messages associated with the following module(s):
-------------------------------------------------------------------------------

mamba:
   - Mamba is a drop-in replacement for conda, offering higher speed and
   more reliable environment solutions. - Do you really need conda/mamba??
   Do you know about python virtual environments??? We HIGHLY recommend
   using virtual envs instead. e.g. python -m venv my-environment - Remember
   you can change where conda environemnts are created (if you run out of
   space in your home directory) ``` $ conda config --env --prepend
   envs_dirs /path/to/my/project/on/fred/.conda/envs $ conda config --env
   --prepend pkgs_dirs /path/to/my/project/on/fred/.conda/pkgs ``` - To hide
   this message in the future, load the module with -q ``` $ module -q load
   conda ```

-------------------------------------------------------------------------------

/fred/oz413/.conda/envs/llm-env/lib/python3.10/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  torch.utils._pytree._register_pytree_node(
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/fred/oz413/.conda/envs/llm-env/lib/python3.10/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  torch.utils._pytree._register_pytree_node(
/fred/oz413/.conda/envs/llm-env/lib/python3.10/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  torch.utils._pytree._register_pytree_node(
/fred/oz413/.conda/envs/llm-env/lib/python3.10/site-packages/peft/tuners/lora.py:475: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.
  warnings.warn(
/fred/oz413/.conda/envs/llm-env/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:166: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024
  warnings.warn(
Map:   0%|          | 0/10000 [00:00<?, ? examples/s]Map:  10%|█         | 1000/10000 [00:00<00:01, 6518.27 examples/s]Map:  30%|███       | 3000/10000 [00:00<00:00, 8714.60 examples/s]Map:  50%|█████     | 5000/10000 [00:00<00:00, 9484.28 examples/s]Map:  70%|███████   | 7000/10000 [00:00<00:00, 9733.17 examples/s]Map:  80%|████████  | 8000/10000 [00:00<00:00, 9539.45 examples/s]Map: 100%|██████████| 10000/10000 [00:01<00:00, 9737.25 examples/s]Map: 100%|██████████| 10000/10000 [00:01<00:00, 9399.30 examples/s]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 9100.21 examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 8878.96 examples/s]
wandb: ERROR api_key not configured (no-tty). call wandb.login(key=[your_api_key])
Traceback (most recent call last):
  File "/fred/oz413/ANeurIPS2024_SPV-MIA/./ft_llms/llms_finetune.py", line 269, in <module>
    trainer.train()
  File "/fred/oz413/.conda/envs/llm-env/lib/python3.10/site-packages/transformers/trainer.py", line 1591, in train
    return inner_training_loop(
  File "/fred/oz413/.conda/envs/llm-env/lib/python3.10/site-packages/transformers/trainer.py", line 1826, in _inner_training_loop
    self.control = self.callback_handler.on_train_begin(args, self.state, self.control)
  File "/fred/oz413/.conda/envs/llm-env/lib/python3.10/site-packages/transformers/trainer_callback.py", line 362, in on_train_begin
    return self.call_event("on_train_begin", args, state, control)
  File "/fred/oz413/.conda/envs/llm-env/lib/python3.10/site-packages/transformers/trainer_callback.py", line 406, in call_event
    result = getattr(callback, event)(
  File "/fred/oz413/.conda/envs/llm-env/lib/python3.10/site-packages/transformers/integrations/integration_utils.py", line 766, in on_train_begin
    self.setup(args, state, model, **kwargs)
  File "/fred/oz413/.conda/envs/llm-env/lib/python3.10/site-packages/transformers/integrations/integration_utils.py", line 740, in setup
    self._wandb.init(
  File "/fred/oz413/.conda/envs/llm-env/lib/python3.10/site-packages/wandb/sdk/wandb_init.py", line 1691, in init
    wandb._sentry.reraise(e)
  File "/fred/oz413/.conda/envs/llm-env/lib/python3.10/site-packages/wandb/analytics/sentry.py", line 156, in reraise
    raise exc.with_traceback(sys.exc_info()[2])
  File "/fred/oz413/.conda/envs/llm-env/lib/python3.10/site-packages/wandb/sdk/wandb_init.py", line 1623, in init
    wi.maybe_login(init_settings)
  File "/fred/oz413/.conda/envs/llm-env/lib/python3.10/site-packages/wandb/sdk/wandb_init.py", line 208, in maybe_login
    wandb_login._login(
  File "/fred/oz413/.conda/envs/llm-env/lib/python3.10/site-packages/wandb/sdk/wandb_login.py", line 314, in _login
    key, key_status = wlogin.prompt_api_key(referrer=referrer)
  File "/fred/oz413/.conda/envs/llm-env/lib/python3.10/site-packages/wandb/sdk/wandb_login.py", line 242, in prompt_api_key
    raise UsageError("api_key not configured (no-tty). call " + directive)
wandb.errors.errors.UsageError: api_key not configured (no-tty). call wandb.login(key=[your_api_key])
Traceback (most recent call last):
  File "/fred/oz413/.conda/envs/llm-env/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/fred/oz413/.conda/envs/llm-env/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 47, in main
    args.func(args)
  File "/fred/oz413/.conda/envs/llm-env/lib/python3.10/site-packages/accelerate/commands/launch.py", line 986, in launch_command
    simple_launcher(args)
  File "/fred/oz413/.conda/envs/llm-env/lib/python3.10/site-packages/accelerate/commands/launch.py", line 628, in simple_launcher
    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)
subprocess.CalledProcessError: Command '['/fred/oz413/.conda/envs/llm-env/bin/python', './ft_llms/llms_finetune.py', '--output_dir', './ft_llms/gpt2-xl/ag_news/baseline/', '--block_size', '128', '--eval_steps', '100', '--save_epochs', '100', '--log_steps', '100', '-d', 'ag_news', '-m', 'gpt2-xl', '--packing', '--use_dataset_cache', '-e', '2', '-b', '4', '-lr', '5e-5', '--gradient_accumulation_steps', '1', '--train_sta_idx=0', '--train_end_idx=10000', '--eval_sta_idx=0', '--eval_end_idx=1000', '--dataset_config_name', 'default']' returned non-zero exit status 1.
