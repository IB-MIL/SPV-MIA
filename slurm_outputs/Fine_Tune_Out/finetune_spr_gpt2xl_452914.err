The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) nvidia/.latest   2) slurm/.latest
-------------------------------------------------------------------------------
There are messages associated with the following module(s):
-------------------------------------------------------------------------------

mamba:
   - Mamba is a drop-in replacement for conda, offering higher speed and
   more reliable environment solutions. - Do you really need conda/mamba??
   Do you know about python virtual environments??? We HIGHLY recommend
   using virtual envs instead. e.g. python -m venv my-environment - Remember
   you can change where conda environemnts are created (if you run out of
   space in your home directory) ``` $ conda config --env --prepend
   envs_dirs /path/to/my/project/on/fred/.conda/envs $ conda config --env
   --prepend pkgs_dirs /path/to/my/project/on/fred/.conda/pkgs ``` - To hide
   this message in the future, load the module with -q ``` $ module -q load
   conda ```

-------------------------------------------------------------------------------

/fred/oz413/.conda/envs/llm-env/lib/python3.10/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  torch.utils._pytree._register_pytree_node(
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/fred/oz413/.conda/envs/llm-env/lib/python3.10/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  torch.utils._pytree._register_pytree_node(
/fred/oz413/.conda/envs/llm-env/lib/python3.10/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  torch.utils._pytree._register_pytree_node(
/fred/oz413/.conda/envs/llm-env/lib/python3.10/site-packages/peft/tuners/lora.py:475: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.
  warnings.warn(
Traceback (most recent call last):
  File "/fred/oz413/ANeurIPS2024_SPV-MIA/./ft_llms/llms_finetune.py", line 219, in <module>
    train_dataset, valid_dataset = dataset_prepare(args, tokenizer=tokenizer)
  File "/fred/oz413/ANeurIPS2024_SPV-MIA/./ft_llms/../data/prepare.py", line 53, in dataset_prepare
    raw_dataset = load_from_disk(dataset_path)
  File "/fred/oz413/.conda/envs/llm-env/lib/python3.10/site-packages/datasets/load.py", line 2252, in load_from_disk
    raise FileNotFoundError(
FileNotFoundError: Directory /fred/oz413/LLM/ag_news/default is neither a `Dataset` directory nor a `DatasetDict` directory.
Traceback (most recent call last):
  File "/fred/oz413/.conda/envs/llm-env/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/fred/oz413/.conda/envs/llm-env/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 47, in main
    args.func(args)
  File "/fred/oz413/.conda/envs/llm-env/lib/python3.10/site-packages/accelerate/commands/launch.py", line 986, in launch_command
    simple_launcher(args)
  File "/fred/oz413/.conda/envs/llm-env/lib/python3.10/site-packages/accelerate/commands/launch.py", line 628, in simple_launcher
    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)
subprocess.CalledProcessError: Command '['/fred/oz413/.conda/envs/llm-env/bin/python', './ft_llms/llms_finetune.py', '--output_dir', './ft_llms/gpt2-xl/ag_news/baseline/', '--block_size', '128', '--eval_steps', '100', '--save_epochs', '100', '--log_steps', '100', '-d', 'ag_news', '-m', 'gpt2-xl', '--packing', '--use_dataset_cache', '-e', '2', '-b', '4', '-lr', '5e-5', '--gradient_accumulation_steps', '1', '--train_sta_idx=0', '--train_end_idx=10000', '--eval_sta_idx=0', '--eval_end_idx=1000', '--dataset_config_name', 'default']' returned non-zero exit status 1.
