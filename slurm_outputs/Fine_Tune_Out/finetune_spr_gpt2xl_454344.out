Job Started: Mon May 19 18:11:49 AEST 2025
Job ID: 454344
Running on node(s): gina3
Allocated CPUs: 8
Allocated Memory: 65536 MB
Allocated GPU(s) Raw: 2 (using GRES: )
CUDA_VISIBLE_DEVICES: 0
Attempting to load system Mamba module...
Conda (mamba) command found at: conda ()
{ 
    \local cmd="${1-__missing__}";
    case "$cmd" in 
        activate | deactivate)
            __conda_activate "$@"
        ;;
        install | update | upgrade | remove | uninstall)
            __conda_exe "$@" || \return;
            __conda_reactivate
        ;;
        *)
            __conda_exe "$@"
        ;;
    esac
}
Activating Conda env: llm-env
Python location: /fred/oz413/.conda/envs/llm-env/bin/python
Python version: Python 3.10.16
PyTorch CUDA available: True
Number of GPUs PyTorch sees: 1
Mon May 19 18:12:26 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.124.06             Driver Version: 570.124.06     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-80GB          On  |   00000000:81:00.0 Off |                    0 |
| N/A   27C    P0             57W /  500W |       1MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Working directory: /fred/oz413/ANeurIPS2024_SPV-MIA
Launching training with Accelerate...
[05/19/25 18:13:52] INFO     Model is not llama, disabling  llms_finetune.py:102
                             flash attention...                                 
                    INFO     Pad token id is None, setting  llms_finetune.py:136
                             to eos token id...                                 
                    INFO     Using a block size of 128      llms_finetune.py:140
                    INFO     Using no quantization          llms_finetune.py:158
[05/19/25 18:14:15] INFO     Using PEFT...                  llms_finetune.py:199
                    INFO     Getting PEFT model...          llms_finetune.py:207
trainable params: 19660800 || all params: 1577272000 || trainable%: 1.2465066266312976
DEBUG: args object in dataset_prepare: Namespace(model_name='gpt2-xl', dataset_name='ag_news', dataset_config_name='default', cache_path='./cache', use_dataset_cache=True, refer=False, refer_data_source=None, packing=True, token=None, split_model=False, block_size=128, preprocessing_num_workers=1, peft='lora', lora_rank=64, lora_alpha=16, lora_dropout=0.1, p_tokens=20, p_hidden=128, learning_rate=5e-05, lr_scheduler_type='linear', warmup_steps=0, weight_decay=0, output_dir='./ft_llms/gpt2-xl/ag_news/baseline/', log_steps=100, eval_steps=100, save_epochs=100, epochs=2, batch_size=4, gradient_accumulation_steps=1, gradient_checkpointing=False, trust_remote_code=False, train_sta_idx=0, train_end_idx=10000, eval_sta_idx=0, eval_end_idx=1000, save_limit=None, use_int4=False, use_int8=False, disable_peft=False, disable_flash_attention=False, pad_token_id=None, add_eos_token=False, add_bos_token=False, validation_split_percentage=0.1)
Found dataset at specific config path: /fred/oz413/LLM/ag_news/default
Loading dataset from disk: /fred/oz413/LLM/ag_news/default
Identified text column as: 'text'
Applying text packing...
Folder './cache/ag_news/default' already exists.
Folder './cache/ag_news/default' already exists.
[05/19/25 18:14:20] INFO     Training with 1 GPUs           llms_finetune.py:228
{'loss': 3.7263, 'learning_rate': 4.9e-05, 'epoch': 0.04}
{'eval_loss': 3.5709779262542725, 'eval_runtime': 8.6778, 'eval_samples_per_second': 115.237, 'eval_steps_per_second': 14.405, 'epoch': 0.04}
{'loss': 3.5429, 'learning_rate': 4.8e-05, 'epoch': 0.08}
{'eval_loss': 3.409566879272461, 'eval_runtime': 8.6304, 'eval_samples_per_second': 115.87, 'eval_steps_per_second': 14.484, 'epoch': 0.08}
{'loss': 3.3964, 'learning_rate': 4.7e-05, 'epoch': 0.12}
{'eval_loss': 3.312258243560791, 'eval_runtime': 9.0911, 'eval_samples_per_second': 109.997, 'eval_steps_per_second': 13.75, 'epoch': 0.12}
{'loss': 3.3637, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.16}
{'eval_loss': 3.256887674331665, 'eval_runtime': 9.1612, 'eval_samples_per_second': 109.156, 'eval_steps_per_second': 13.645, 'epoch': 0.16}
{'loss': 3.3024, 'learning_rate': 4.5e-05, 'epoch': 0.2}
{'eval_loss': 3.2213404178619385, 'eval_runtime': 9.1449, 'eval_samples_per_second': 109.35, 'eval_steps_per_second': 13.669, 'epoch': 0.2}
{'loss': 3.2872, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.24}
{'eval_loss': 3.1933956146240234, 'eval_runtime': 8.6453, 'eval_samples_per_second': 115.67, 'eval_steps_per_second': 14.459, 'epoch': 0.24}
{'loss': 3.2707, 'learning_rate': 4.3e-05, 'epoch': 0.28}
{'eval_loss': 3.1726787090301514, 'eval_runtime': 8.6348, 'eval_samples_per_second': 115.811, 'eval_steps_per_second': 14.476, 'epoch': 0.28}
{'loss': 3.2197, 'learning_rate': 4.2e-05, 'epoch': 0.32}
{'eval_loss': 3.1539812088012695, 'eval_runtime': 8.6774, 'eval_samples_per_second': 115.242, 'eval_steps_per_second': 14.405, 'epoch': 0.32}
{'loss': 3.2565, 'learning_rate': 4.1e-05, 'epoch': 0.36}
{'eval_loss': 3.1418726444244385, 'eval_runtime': 8.6504, 'eval_samples_per_second': 115.602, 'eval_steps_per_second': 14.45, 'epoch': 0.36}
{'loss': 3.2367, 'learning_rate': 4e-05, 'epoch': 0.4}
{'eval_loss': 3.131356954574585, 'eval_runtime': 8.6405, 'eval_samples_per_second': 115.734, 'eval_steps_per_second': 14.467, 'epoch': 0.4}
{'loss': 3.1841, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.44}
{'eval_loss': 3.1190502643585205, 'eval_runtime': 8.6457, 'eval_samples_per_second': 115.664, 'eval_steps_per_second': 14.458, 'epoch': 0.44}
{'loss': 3.191, 'learning_rate': 3.8e-05, 'epoch': 0.48}
{'eval_loss': 3.1096582412719727, 'eval_runtime': 8.644, 'eval_samples_per_second': 115.687, 'eval_steps_per_second': 14.461, 'epoch': 0.48}
{'loss': 3.2306, 'learning_rate': 3.7e-05, 'epoch': 0.52}
{'eval_loss': 3.102724075317383, 'eval_runtime': 8.677, 'eval_samples_per_second': 115.247, 'eval_steps_per_second': 14.406, 'epoch': 0.52}
{'loss': 3.187, 'learning_rate': 3.6e-05, 'epoch': 0.56}
{'eval_loss': 3.0950393676757812, 'eval_runtime': 8.6345, 'eval_samples_per_second': 115.815, 'eval_steps_per_second': 14.477, 'epoch': 0.56}
{'loss': 3.1385, 'learning_rate': 3.5e-05, 'epoch': 0.6}
{'eval_loss': 3.0896852016448975, 'eval_runtime': 8.6329, 'eval_samples_per_second': 115.836, 'eval_steps_per_second': 14.479, 'epoch': 0.6}
{'loss': 3.1808, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.64}
{'eval_loss': 3.0841856002807617, 'eval_runtime': 8.6606, 'eval_samples_per_second': 115.466, 'eval_steps_per_second': 14.433, 'epoch': 0.64}
{'loss': 3.1383, 'learning_rate': 3.3e-05, 'epoch': 0.68}
{'eval_loss': 3.0806543827056885, 'eval_runtime': 8.627, 'eval_samples_per_second': 115.916, 'eval_steps_per_second': 14.489, 'epoch': 0.68}
{'loss': 3.1481, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.72}
{'eval_loss': 3.072537422180176, 'eval_runtime': 8.663, 'eval_samples_per_second': 115.433, 'eval_steps_per_second': 14.429, 'epoch': 0.72}
{'loss': 3.1161, 'learning_rate': 3.1e-05, 'epoch': 0.76}
{'eval_loss': 3.068696975708008, 'eval_runtime': 8.6451, 'eval_samples_per_second': 115.673, 'eval_steps_per_second': 14.459, 'epoch': 0.76}
{'loss': 3.1504, 'learning_rate': 3e-05, 'epoch': 0.8}
{'eval_loss': 3.0656089782714844, 'eval_runtime': 8.6287, 'eval_samples_per_second': 115.893, 'eval_steps_per_second': 14.487, 'epoch': 0.8}
{'loss': 3.1513, 'learning_rate': 2.9e-05, 'epoch': 0.84}
{'eval_loss': 3.059546947479248, 'eval_runtime': 8.6494, 'eval_samples_per_second': 115.615, 'eval_steps_per_second': 14.452, 'epoch': 0.84}
{'loss': 3.1411, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.88}
{'eval_loss': 3.055443048477173, 'eval_runtime': 8.641, 'eval_samples_per_second': 115.728, 'eval_steps_per_second': 14.466, 'epoch': 0.88}
{'loss': 3.1036, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.92}
{'eval_loss': 3.0531299114227295, 'eval_runtime': 8.6445, 'eval_samples_per_second': 115.68, 'eval_steps_per_second': 14.46, 'epoch': 0.92}
{'loss': 3.1258, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.96}
{'eval_loss': 3.0508980751037598, 'eval_runtime': 8.6558, 'eval_samples_per_second': 115.53, 'eval_steps_per_second': 14.441, 'epoch': 0.96}
{'loss': 3.1085, 'learning_rate': 2.5e-05, 'epoch': 1.0}
{'eval_loss': 3.0486650466918945, 'eval_runtime': 8.647, 'eval_samples_per_second': 115.647, 'eval_steps_per_second': 14.456, 'epoch': 1.0}
{'loss': 3.114, 'learning_rate': 2.4e-05, 'epoch': 1.04}
{'eval_loss': 3.04534912109375, 'eval_runtime': 8.6257, 'eval_samples_per_second': 115.933, 'eval_steps_per_second': 14.492, 'epoch': 1.04}
{'loss': 3.1156, 'learning_rate': 2.3000000000000003e-05, 'epoch': 1.08}
{'eval_loss': 3.044407606124878, 'eval_runtime': 8.6752, 'eval_samples_per_second': 115.271, 'eval_steps_per_second': 14.409, 'epoch': 1.08}
{'loss': 3.1237, 'learning_rate': 2.2000000000000003e-05, 'epoch': 1.12}
{'eval_loss': 3.040449380874634, 'eval_runtime': 8.6277, 'eval_samples_per_second': 115.906, 'eval_steps_per_second': 14.488, 'epoch': 1.12}
{'loss': 3.1298, 'learning_rate': 2.1e-05, 'epoch': 1.16}
{'eval_loss': 3.0394017696380615, 'eval_runtime': 8.6434, 'eval_samples_per_second': 115.695, 'eval_steps_per_second': 14.462, 'epoch': 1.16}
{'loss': 3.1148, 'learning_rate': 2e-05, 'epoch': 1.2}
{'eval_loss': 3.03780460357666, 'eval_runtime': 8.622, 'eval_samples_per_second': 115.982, 'eval_steps_per_second': 14.498, 'epoch': 1.2}
{'loss': 3.0885, 'learning_rate': 1.9e-05, 'epoch': 1.24}
{'eval_loss': 3.036642551422119, 'eval_runtime': 8.6531, 'eval_samples_per_second': 115.566, 'eval_steps_per_second': 14.446, 'epoch': 1.24}
{'loss': 3.1034, 'learning_rate': 1.8e-05, 'epoch': 1.28}
{'eval_loss': 3.035015344619751, 'eval_runtime': 8.6454, 'eval_samples_per_second': 115.668, 'eval_steps_per_second': 14.458, 'epoch': 1.28}
{'loss': 3.0616, 'learning_rate': 1.7000000000000003e-05, 'epoch': 1.32}
{'eval_loss': 3.0336782932281494, 'eval_runtime': 8.6402, 'eval_samples_per_second': 115.738, 'eval_steps_per_second': 14.467, 'epoch': 1.32}
{'loss': 3.0715, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.36}
{'eval_loss': 3.032893180847168, 'eval_runtime': 8.6541, 'eval_samples_per_second': 115.552, 'eval_steps_per_second': 14.444, 'epoch': 1.36}
{'loss': 3.1194, 'learning_rate': 1.5e-05, 'epoch': 1.4}
{'eval_loss': 3.031639575958252, 'eval_runtime': 8.6586, 'eval_samples_per_second': 115.492, 'eval_steps_per_second': 14.436, 'epoch': 1.4}
{'loss': 3.0337, 'learning_rate': 1.4000000000000001e-05, 'epoch': 1.44}
{'eval_loss': 3.0305259227752686, 'eval_runtime': 8.6428, 'eval_samples_per_second': 115.703, 'eval_steps_per_second': 14.463, 'epoch': 1.44}
{'loss': 3.0425, 'learning_rate': 1.3000000000000001e-05, 'epoch': 1.48}
{'eval_loss': 3.0289251804351807, 'eval_runtime': 8.6423, 'eval_samples_per_second': 115.71, 'eval_steps_per_second': 14.464, 'epoch': 1.48}
{'loss': 3.0985, 'learning_rate': 1.2e-05, 'epoch': 1.52}
{'eval_loss': 3.027757406234741, 'eval_runtime': 8.672, 'eval_samples_per_second': 115.313, 'eval_steps_per_second': 14.414, 'epoch': 1.52}
{'loss': 3.1314, 'learning_rate': 1.1000000000000001e-05, 'epoch': 1.56}
{'eval_loss': 3.0258336067199707, 'eval_runtime': 8.632, 'eval_samples_per_second': 115.847, 'eval_steps_per_second': 14.481, 'epoch': 1.56}
{'loss': 3.0588, 'learning_rate': 1e-05, 'epoch': 1.6}
{'eval_loss': 3.025275230407715, 'eval_runtime': 8.6273, 'eval_samples_per_second': 115.911, 'eval_steps_per_second': 14.489, 'epoch': 1.6}
{'loss': 3.1133, 'learning_rate': 9e-06, 'epoch': 1.64}
{'eval_loss': 3.0238208770751953, 'eval_runtime': 8.655, 'eval_samples_per_second': 115.539, 'eval_steps_per_second': 14.442, 'epoch': 1.64}
{'loss': 3.0241, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.68}
{'eval_loss': 3.0238852500915527, 'eval_runtime': 8.6296, 'eval_samples_per_second': 115.88, 'eval_steps_per_second': 14.485, 'epoch': 1.68}
{'loss': 3.0727, 'learning_rate': 7.000000000000001e-06, 'epoch': 1.72}
{'eval_loss': 3.022998332977295, 'eval_runtime': 8.6403, 'eval_samples_per_second': 115.737, 'eval_steps_per_second': 14.467, 'epoch': 1.72}
{'loss': 3.0611, 'learning_rate': 6e-06, 'epoch': 1.76}
{'eval_loss': 3.022886037826538, 'eval_runtime': 9.2443, 'eval_samples_per_second': 108.174, 'eval_steps_per_second': 13.522, 'epoch': 1.76}
{'loss': 3.0685, 'learning_rate': 5e-06, 'epoch': 1.8}
{'eval_loss': 3.022261619567871, 'eval_runtime': 8.6354, 'eval_samples_per_second': 115.802, 'eval_steps_per_second': 14.475, 'epoch': 1.8}
{'loss': 3.1053, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.84}
{'eval_loss': 3.0217864513397217, 'eval_runtime': 8.6379, 'eval_samples_per_second': 115.769, 'eval_steps_per_second': 14.471, 'epoch': 1.84}
{'loss': 3.0435, 'learning_rate': 3e-06, 'epoch': 1.88}
{'eval_loss': 3.021272897720337, 'eval_runtime': 8.6407, 'eval_samples_per_second': 115.732, 'eval_steps_per_second': 14.466, 'epoch': 1.88}
{'loss': 3.0961, 'learning_rate': 2.0000000000000003e-06, 'epoch': 1.92}
{'eval_loss': 3.021308183670044, 'eval_runtime': 8.6457, 'eval_samples_per_second': 115.664, 'eval_steps_per_second': 14.458, 'epoch': 1.92}
{'loss': 3.0876, 'learning_rate': 1.0000000000000002e-06, 'epoch': 1.96}
{'eval_loss': 3.0211679935455322, 'eval_runtime': 8.6429, 'eval_samples_per_second': 115.701, 'eval_steps_per_second': 14.463, 'epoch': 1.96}
{'loss': 3.098, 'learning_rate': 0.0, 'epoch': 2.0}
{'eval_loss': 3.021031141281128, 'eval_runtime': 8.6499, 'eval_samples_per_second': 115.609, 'eval_steps_per_second': 14.451, 'epoch': 2.0}
{'train_runtime': 1132.8022, 'train_samples_per_second': 17.655, 'train_steps_per_second': 4.414, 'train_loss': 3.1615016906738282, 'epoch': 2.0}
Fine-tuning completed successfully.
Job Ended: Mon May 19 18:33:37 AEST 2025

+------------------- Job Report: 454344 (COMPLETED) -------------------+
| Memory (RAM)  [##                  ] 10.4% (6.6 GB peak / 64 GB)     |
| CPU           [##                  ] 10.6% average                   |
| GPU           [############        ] 60.8% average                   |
| Time          [------->            ] 36.7% (0-00:22:02 / 0-01:00:00) |
|                                                                      |
| Lustre Filesystem:                                                   |
|   Path    Total Read    Total Write    Total IOPS                    |
|   ------  ------------  -------------  ------------                  |
|   /apps   25.6 MB       0 B            2 K                           |
|   /fred   508.2 MB      10.9 GB        24.2 K                        |
|   /home   8 KB          0 B            15                            |
|   OS      16.9 MB       0 B            25                            |
|                                                                      |
| Warnings:                                                            |
|   - Too much memory requested                                        |
|   - CPU usage is low                                                 |
|   - GPU usage is low                                                 |
|   - Too much time requested                                          |
+----------------------------------------------------------------------+
