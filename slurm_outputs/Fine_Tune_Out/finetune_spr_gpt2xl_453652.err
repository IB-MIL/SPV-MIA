The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) nvidia/.latest   2) slurm/.latest
-------------------------------------------------------------------------------
There are messages associated with the following module(s):
-------------------------------------------------------------------------------

mamba:
   - Mamba is a drop-in replacement for conda, offering higher speed and
   more reliable environment solutions. - Do you really need conda/mamba??
   Do you know about python virtual environments??? We HIGHLY recommend
   using virtual envs instead. e.g. python -m venv my-environment - Remember
   you can change where conda environemnts are created (if you run out of
   space in your home directory) ``` $ conda config --env --prepend
   envs_dirs /path/to/my/project/on/fred/.conda/envs $ conda config --env
   --prepend pkgs_dirs /path/to/my/project/on/fred/.conda/pkgs ``` - To hide
   this message in the future, load the module with -q ``` $ module -q load
   conda ```

-------------------------------------------------------------------------------

/fred/oz413/.conda/envs/llm-env/lib/python3.10/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  torch.utils._pytree._register_pytree_node(
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/fred/oz413/.conda/envs/llm-env/lib/python3.10/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  torch.utils._pytree._register_pytree_node(
/fred/oz413/.conda/envs/llm-env/lib/python3.10/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  torch.utils._pytree._register_pytree_node(
/fred/oz413/.conda/envs/llm-env/lib/python3.10/site-packages/peft/tuners/lora.py:475: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.
  warnings.warn(
Packing train texts in chunks of 128 tokens:   0%|          | 0/120000 [00:00<?, ? examples/s]Packing train texts in chunks of 128 tokens:   1%|          | 1000/120000 [00:00<00:46, 2561.45 examples/s]Packing train texts in chunks of 128 tokens:   2%|▏         | 2000/120000 [00:00<00:35, 3331.66 examples/s]Packing train texts in chunks of 128 tokens:   2%|▎         | 3000/120000 [00:00<00:30, 3872.30 examples/s]Packing train texts in chunks of 128 tokens:   3%|▎         | 4000/120000 [00:01<00:28, 4095.98 examples/s]Packing train texts in chunks of 128 tokens:   4%|▍         | 5000/120000 [00:01<00:26, 4336.15 examples/s]Packing train texts in chunks of 128 tokens:   5%|▌         | 6000/120000 [00:01<00:26, 4340.02 examples/s]Packing train texts in chunks of 128 tokens:   6%|▌         | 7000/120000 [00:01<00:26, 4243.44 examples/s]Packing train texts in chunks of 128 tokens:   7%|▋         | 8000/120000 [00:01<00:26, 4181.58 examples/s]Packing train texts in chunks of 128 tokens:   8%|▊         | 9000/120000 [00:02<00:26, 4195.20 examples/s]Packing train texts in chunks of 128 tokens:   8%|▊         | 10000/120000 [00:02<00:26, 4174.34 examples/s]Packing train texts in chunks of 128 tokens:   9%|▉         | 11000/120000 [00:02<00:26, 4092.48 examples/s]Packing train texts in chunks of 128 tokens:  10%|█         | 12000/120000 [00:02<00:26, 4078.71 examples/s]Packing train texts in chunks of 128 tokens:  11%|█         | 13000/120000 [00:03<00:26, 4086.41 examples/s]Packing train texts in chunks of 128 tokens:  12%|█▏        | 14000/120000 [00:03<00:25, 4176.05 examples/s]Packing train texts in chunks of 128 tokens:  12%|█▎        | 15000/120000 [00:03<00:24, 4222.46 examples/s]Packing train texts in chunks of 128 tokens:  13%|█▎        | 16000/120000 [00:04<00:30, 3377.68 examples/s]Packing train texts in chunks of 128 tokens:  14%|█▍        | 17000/120000 [00:04<00:29, 3526.90 examples/s]Packing train texts in chunks of 128 tokens:  15%|█▌        | 18000/120000 [00:04<00:27, 3682.52 examples/s]Packing train texts in chunks of 128 tokens:  16%|█▌        | 19000/120000 [00:04<00:26, 3802.10 examples/s]Packing train texts in chunks of 128 tokens:  17%|█▋        | 20000/120000 [00:05<00:25, 3970.42 examples/s]Packing train texts in chunks of 128 tokens:  18%|█▊        | 21000/120000 [00:05<00:24, 4037.16 examples/s]Packing train texts in chunks of 128 tokens:  18%|█▊        | 22000/120000 [00:05<00:24, 4016.22 examples/s]Packing train texts in chunks of 128 tokens:  19%|█▉        | 23000/120000 [00:05<00:23, 4081.25 examples/s]Packing train texts in chunks of 128 tokens:  20%|██        | 24000/120000 [00:06<00:23, 4108.33 examples/s]Packing train texts in chunks of 128 tokens:  21%|██        | 25000/120000 [00:06<00:22, 4150.10 examples/s]Packing train texts in chunks of 128 tokens:  22%|██▏       | 26000/120000 [00:06<00:23, 4060.81 examples/s]Packing train texts in chunks of 128 tokens:  22%|██▎       | 27000/120000 [00:06<00:22, 4096.90 examples/s]Packing train texts in chunks of 128 tokens:  23%|██▎       | 28000/120000 [00:07<00:22, 4161.79 examples/s]Packing train texts in chunks of 128 tokens:  24%|██▍       | 29000/120000 [00:07<00:21, 4145.12 examples/s]Packing train texts in chunks of 128 tokens:  25%|██▌       | 30000/120000 [00:07<00:21, 4117.50 examples/s]Packing train texts in chunks of 128 tokens:  26%|██▌       | 31000/120000 [00:07<00:21, 4058.45 examples/s]Packing train texts in chunks of 128 tokens:  27%|██▋       | 32000/120000 [00:07<00:21, 4145.73 examples/s]Packing train texts in chunks of 128 tokens:  28%|██▊       | 33000/120000 [00:08<00:20, 4168.30 examples/s]Packing train texts in chunks of 128 tokens:  28%|██▊       | 34000/120000 [00:08<00:20, 4159.24 examples/s]Packing train texts in chunks of 128 tokens:  29%|██▉       | 35000/120000 [00:08<00:20, 4082.48 examples/s]Packing train texts in chunks of 128 tokens:  30%|███       | 36000/120000 [00:08<00:20, 4091.30 examples/s]Packing train texts in chunks of 128 tokens:  31%|███       | 37000/120000 [00:09<00:19, 4177.65 examples/s]Packing train texts in chunks of 128 tokens:  32%|███▏      | 38000/120000 [00:09<00:19, 4244.99 examples/s]Packing train texts in chunks of 128 tokens:  32%|███▎      | 39000/120000 [00:09<00:19, 4260.97 examples/s]Packing train texts in chunks of 128 tokens:  33%|███▎      | 40000/120000 [00:09<00:18, 4274.07 examples/s]Packing train texts in chunks of 128 tokens:  34%|███▍      | 41000/120000 [00:10<00:18, 4270.13 examples/s]Packing train texts in chunks of 128 tokens:  35%|███▌      | 42000/120000 [00:10<00:18, 4237.56 examples/s]Packing train texts in chunks of 128 tokens:  36%|███▌      | 43000/120000 [00:10<00:18, 4184.76 examples/s]Packing train texts in chunks of 128 tokens:  37%|███▋      | 44000/120000 [00:10<00:18, 4146.83 examples/s]Packing train texts in chunks of 128 tokens:  38%|███▊      | 45000/120000 [00:11<00:18, 4123.87 examples/s]Packing train texts in chunks of 128 tokens:  38%|███▊      | 46000/120000 [00:11<00:17, 4174.42 examples/s]Packing train texts in chunks of 128 tokens:  39%|███▉      | 47000/120000 [00:11<00:22, 3308.31 examples/s]Packing train texts in chunks of 128 tokens:  40%|████      | 48000/120000 [00:12<00:20, 3491.98 examples/s]Packing train texts in chunks of 128 tokens:  41%|████      | 49000/120000 [00:12<00:19, 3645.67 examples/s]Packing train texts in chunks of 128 tokens:  42%|████▏     | 50000/120000 [00:12<00:18, 3757.87 examples/s]Packing train texts in chunks of 128 tokens:  42%|████▎     | 51000/120000 [00:12<00:17, 3903.13 examples/s]Packing train texts in chunks of 128 tokens:  43%|████▎     | 52000/120000 [00:12<00:17, 3906.67 examples/s]Packing train texts in chunks of 128 tokens:  44%|████▍     | 53000/120000 [00:13<00:16, 4069.60 examples/s]Packing train texts in chunks of 128 tokens:  45%|████▌     | 54000/120000 [00:13<00:15, 4154.19 examples/s]Packing train texts in chunks of 128 tokens:  46%|████▌     | 55000/120000 [00:13<00:15, 4125.74 examples/s]Packing train texts in chunks of 128 tokens:  47%|████▋     | 56000/120000 [00:13<00:15, 4105.53 examples/s]Packing train texts in chunks of 128 tokens:  48%|████▊     | 57000/120000 [00:14<00:15, 4099.42 examples/s]Packing train texts in chunks of 128 tokens:  48%|████▊     | 58000/120000 [00:14<00:15, 4086.96 examples/s]Packing train texts in chunks of 128 tokens:  49%|████▉     | 59000/120000 [00:14<00:15, 4020.92 examples/s]Packing train texts in chunks of 128 tokens:  50%|█████     | 60000/120000 [00:14<00:14, 4064.44 examples/s]Packing train texts in chunks of 128 tokens:  51%|█████     | 61000/120000 [00:15<00:14, 4100.47 examples/s]Packing train texts in chunks of 128 tokens:  52%|█████▏    | 62000/120000 [00:15<00:14, 4129.51 examples/s]Packing train texts in chunks of 128 tokens:  52%|█████▎    | 63000/120000 [00:15<00:14, 4068.75 examples/s]Packing train texts in chunks of 128 tokens:  53%|█████▎    | 64000/120000 [00:15<00:13, 4016.52 examples/s]Packing train texts in chunks of 128 tokens:  54%|█████▍    | 65000/120000 [00:16<00:13, 4075.67 examples/s]Packing train texts in chunks of 128 tokens:  55%|█████▌    | 66000/120000 [00:16<00:13, 4019.30 examples/s]Packing train texts in chunks of 128 tokens:  56%|█████▌    | 67000/120000 [00:16<00:13, 4067.53 examples/s]Packing train texts in chunks of 128 tokens:  57%|█████▋    | 68000/120000 [00:16<00:12, 4058.05 examples/s]Packing train texts in chunks of 128 tokens:  57%|█████▊    | 69000/120000 [00:17<00:12, 4051.92 examples/s]Packing train texts in chunks of 128 tokens:  58%|█████▊    | 70000/120000 [00:17<00:12, 4092.68 examples/s]Packing train texts in chunks of 128 tokens:  59%|█████▉    | 71000/120000 [00:17<00:11, 4130.10 examples/s]Packing train texts in chunks of 128 tokens:  60%|██████    | 72000/120000 [00:17<00:11, 4121.27 examples/s]Packing train texts in chunks of 128 tokens:  61%|██████    | 73000/120000 [00:18<00:11, 4066.07 examples/s]Packing train texts in chunks of 128 tokens:  62%|██████▏   | 74000/120000 [00:18<00:11, 4098.01 examples/s]Packing train texts in chunks of 128 tokens:  62%|██████▎   | 75000/120000 [00:18<00:10, 4122.56 examples/s]Packing train texts in chunks of 128 tokens:  63%|██████▎   | 76000/120000 [00:18<00:10, 4177.25 examples/s]Packing train texts in chunks of 128 tokens:  64%|██████▍   | 77000/120000 [00:19<00:13, 3288.31 examples/s]Packing train texts in chunks of 128 tokens:  65%|██████▌   | 78000/120000 [00:19<00:12, 3452.40 examples/s]Packing train texts in chunks of 128 tokens:  66%|██████▌   | 79000/120000 [00:19<00:11, 3659.20 examples/s]Packing train texts in chunks of 128 tokens:  67%|██████▋   | 80000/120000 [00:20<00:10, 3797.58 examples/s]Packing train texts in chunks of 128 tokens:  68%|██████▊   | 81000/120000 [00:20<00:10, 3889.35 examples/s]Packing train texts in chunks of 128 tokens:  68%|██████▊   | 82000/120000 [00:20<00:09, 4021.17 examples/s]Packing train texts in chunks of 128 tokens:  69%|██████▉   | 83000/120000 [00:20<00:09, 4086.93 examples/s]Packing train texts in chunks of 128 tokens:  70%|███████   | 84000/120000 [00:20<00:08, 4068.36 examples/s]Packing train texts in chunks of 128 tokens:  71%|███████   | 85000/120000 [00:21<00:08, 4063.15 examples/s]Packing train texts in chunks of 128 tokens:  72%|███████▏  | 86000/120000 [00:21<00:08, 4041.45 examples/s]Packing train texts in chunks of 128 tokens:  72%|███████▎  | 87000/120000 [00:21<00:08, 4047.78 examples/s]Packing train texts in chunks of 128 tokens:  73%|███████▎  | 88000/120000 [00:21<00:07, 4054.20 examples/s]Packing train texts in chunks of 128 tokens:  74%|███████▍  | 89000/120000 [00:22<00:07, 4027.06 examples/s]Packing train texts in chunks of 128 tokens:  75%|███████▌  | 90000/120000 [00:22<00:07, 4055.24 examples/s]Packing train texts in chunks of 128 tokens:  76%|███████▌  | 91000/120000 [00:22<00:07, 4091.05 examples/s]Packing train texts in chunks of 128 tokens:  77%|███████▋  | 92000/120000 [00:22<00:06, 4098.60 examples/s]Packing train texts in chunks of 128 tokens:  78%|███████▊  | 93000/120000 [00:23<00:06, 4018.30 examples/s]Packing train texts in chunks of 128 tokens:  78%|███████▊  | 94000/120000 [00:23<00:06, 4150.18 examples/s]Packing train texts in chunks of 128 tokens:  79%|███████▉  | 95000/120000 [00:23<00:05, 4219.01 examples/s]Packing train texts in chunks of 128 tokens:  80%|████████  | 96000/120000 [00:23<00:05, 4173.27 examples/s]Packing train texts in chunks of 128 tokens:  81%|████████  | 97000/120000 [00:24<00:05, 4141.28 examples/s]Packing train texts in chunks of 128 tokens:  82%|████████▏ | 98000/120000 [00:24<00:05, 4134.32 examples/s]Packing train texts in chunks of 128 tokens:  82%|████████▎ | 99000/120000 [00:24<00:05, 4195.63 examples/s]Packing train texts in chunks of 128 tokens:  83%|████████▎ | 100000/120000 [00:24<00:04, 4195.62 examples/s]Packing train texts in chunks of 128 tokens:  84%|████████▍ | 101000/120000 [00:25<00:04, 4243.85 examples/s]Packing train texts in chunks of 128 tokens:  85%|████████▌ | 102000/120000 [00:25<00:04, 4237.50 examples/s]Packing train texts in chunks of 128 tokens:  86%|████████▌ | 103000/120000 [00:25<00:03, 4262.66 examples/s]Packing train texts in chunks of 128 tokens:  87%|████████▋ | 104000/120000 [00:25<00:03, 4218.11 examples/s]Packing train texts in chunks of 128 tokens:  88%|████████▊ | 105000/120000 [00:26<00:03, 4122.89 examples/s]Packing train texts in chunks of 128 tokens:  88%|████████▊ | 106000/120000 [00:26<00:03, 4078.77 examples/s]Packing train texts in chunks of 128 tokens:  89%|████████▉ | 107000/120000 [00:26<00:03, 3261.44 examples/s]Packing train texts in chunks of 128 tokens:  90%|█████████ | 108000/120000 [00:26<00:03, 3508.31 examples/s]Packing train texts in chunks of 128 tokens:  91%|█████████ | 109000/120000 [00:27<00:02, 3744.43 examples/s]Packing train texts in chunks of 128 tokens:  92%|█████████▏| 110000/120000 [00:27<00:02, 3839.03 examples/s]Packing train texts in chunks of 128 tokens:  92%|█████████▎| 111000/120000 [00:27<00:02, 3951.27 examples/s]Packing train texts in chunks of 128 tokens:  93%|█████████▎| 112000/120000 [00:27<00:02, 3953.59 examples/s]Packing train texts in chunks of 128 tokens:  94%|█████████▍| 113000/120000 [00:28<00:01, 4021.84 examples/s]Packing train texts in chunks of 128 tokens:  95%|█████████▌| 114000/120000 [00:28<00:01, 4060.20 examples/s]Packing train texts in chunks of 128 tokens:  96%|█████████▌| 115000/120000 [00:28<00:01, 4101.84 examples/s]Packing train texts in chunks of 128 tokens:  97%|█████████▋| 116000/120000 [00:28<00:00, 4101.27 examples/s]Packing train texts in chunks of 128 tokens:  98%|█████████▊| 117000/120000 [00:29<00:00, 4113.92 examples/s]Packing train texts in chunks of 128 tokens:  98%|█████████▊| 118000/120000 [00:29<00:00, 4077.05 examples/s]Packing train texts in chunks of 128 tokens:  99%|█████████▉| 119000/120000 [00:29<00:00, 4108.33 examples/s]Packing train texts in chunks of 128 tokens: 100%|██████████| 120000/120000 [00:29<00:00, 4111.39 examples/s]Packing train texts in chunks of 128 tokens: 100%|██████████| 120000/120000 [00:29<00:00, 4015.67 examples/s]
Packing valid texts in chunks of 128 tokens:   0%|          | 0/7600 [00:00<?, ? examples/s]Packing valid texts in chunks of 128 tokens:  13%|█▎        | 1000/7600 [00:00<00:01, 3663.41 examples/s]Packing valid texts in chunks of 128 tokens:  26%|██▋       | 2000/7600 [00:00<00:01, 4020.26 examples/s]Packing valid texts in chunks of 128 tokens:  39%|███▉      | 3000/7600 [00:00<00:01, 3976.02 examples/s]Packing valid texts in chunks of 128 tokens:  53%|█████▎    | 4000/7600 [00:00<00:00, 4047.47 examples/s]Packing valid texts in chunks of 128 tokens:  66%|██████▌   | 5000/7600 [00:01<00:00, 4105.96 examples/s]Packing valid texts in chunks of 128 tokens:  79%|███████▉  | 6000/7600 [00:01<00:00, 4093.02 examples/s]Packing valid texts in chunks of 128 tokens:  92%|█████████▏| 7000/7600 [00:01<00:00, 4051.53 examples/s]Packing valid texts in chunks of 128 tokens: 100%|██████████| 7600/7600 [00:01<00:00, 4135.86 examples/s]Packing valid texts in chunks of 128 tokens: 100%|██████████| 7600/7600 [00:01<00:00, 4050.79 examples/s]
/fred/oz413/.conda/envs/llm-env/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:166: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024
  warnings.warn(
Map:   0%|          | 0/10000 [00:00<?, ? examples/s]Map:  10%|█         | 1000/10000 [00:00<00:01, 8539.27 examples/s]Map:  30%|███       | 3000/10000 [00:00<00:00, 10485.07 examples/s]Map:  50%|█████     | 5000/10000 [00:00<00:00, 9895.53 examples/s] Map:  60%|██████    | 6000/10000 [00:00<00:00, 9805.05 examples/s]Map:  80%|████████  | 8000/10000 [00:00<00:00, 10277.04 examples/s]Map: 100%|██████████| 10000/10000 [00:01<00:00, 7958.07 examples/s]Map: 100%|██████████| 10000/10000 [00:01<00:00, 8786.65 examples/s]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 8926.94 examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 8593.62 examples/s]
Traceback (most recent call last):
  File "/fred/oz413/ANeurIPS2024_SPV-MIA/./ft_llms/llms_finetune.py", line 259, in <module>
    trainer = SFTTrainer(
  File "/fred/oz413/.conda/envs/llm-env/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 212, in __init__
    super().__init__(
  File "/fred/oz413/.conda/envs/llm-env/lib/python3.10/site-packages/transformers/trainer.py", line 557, in __init__
    self.callback_handler = CallbackHandler(
  File "/fred/oz413/.conda/envs/llm-env/lib/python3.10/site-packages/transformers/trainer_callback.py", line 305, in __init__
    self.add_callback(cb)
  File "/fred/oz413/.conda/envs/llm-env/lib/python3.10/site-packages/transformers/trainer_callback.py", line 322, in add_callback
    cb = callback() if isinstance(callback, type) else callback
  File "/fred/oz413/.conda/envs/llm-env/lib/python3.10/site-packages/transformers/integrations/integration_utils.py", line 673, in __init__
    raise RuntimeError("WandbCallback requires wandb to be installed. Run `pip install wandb`.")
RuntimeError: WandbCallback requires wandb to be installed. Run `pip install wandb`.
Traceback (most recent call last):
  File "/fred/oz413/.conda/envs/llm-env/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/fred/oz413/.conda/envs/llm-env/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 47, in main
    args.func(args)
  File "/fred/oz413/.conda/envs/llm-env/lib/python3.10/site-packages/accelerate/commands/launch.py", line 986, in launch_command
    simple_launcher(args)
  File "/fred/oz413/.conda/envs/llm-env/lib/python3.10/site-packages/accelerate/commands/launch.py", line 628, in simple_launcher
    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)
subprocess.CalledProcessError: Command '['/fred/oz413/.conda/envs/llm-env/bin/python', './ft_llms/llms_finetune.py', '--output_dir', './ft_llms/gpt2-xl/ag_news/baseline/', '--block_size', '128', '--eval_steps', '100', '--save_epochs', '100', '--log_steps', '100', '-d', 'ag_news', '-m', 'gpt2-xl', '--packing', '--use_dataset_cache', '-e', '2', '-b', '4', '-lr', '5e-5', '--gradient_accumulation_steps', '1', '--train_sta_idx=0', '--train_end_idx=10000', '--eval_sta_idx=0', '--eval_end_idx=1000', '--dataset_config_name', 'default']' returned non-zero exit status 1.
