The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) nvidia/.latest   2) slurm/.latest
-------------------------------------------------------------------------------
There are messages associated with the following module(s):
-------------------------------------------------------------------------------

mamba:
   - Mamba is a drop-in replacement for conda, offering higher speed and
   more reliable environment solutions. - Do you really need conda/mamba??
   Do you know about python virtual environments??? We HIGHLY recommend
   using virtual envs instead. e.g. python -m venv my-environment - Remember
   you can change where conda environemnts are created (if you run out of
   space in your home directory) ``` $ conda config --env --prepend
   envs_dirs /path/to/my/project/on/fred/.conda/envs $ conda config --env
   --prepend pkgs_dirs /path/to/my/project/on/fred/.conda/pkgs ``` - To hide
   this message in the future, load the module with -q ``` $ module -q load
   conda ```

-------------------------------------------------------------------------------

/fred/oz413/.conda/envs/llm-env/lib/python3.10/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  torch.utils._pytree._register_pytree_node(
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/fred/oz413/.conda/envs/llm-env/lib/python3.10/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  torch.utils._pytree._register_pytree_node(
/fred/oz413/.conda/envs/llm-env/lib/python3.10/site-packages/accelerate/accelerator.py:437: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.scaler = torch.cuda.amp.GradScaler(**kwargs)
/fred/oz413/.conda/envs/llm-env/lib/python3.10/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  torch.utils._pytree._register_pytree_node(
/fred/oz413/.conda/envs/llm-env/lib/python3.10/site-packages/peft/tuners/lora.py:475: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.
  warnings.warn(
Map:   0%|          | 0/10000 [00:00<?, ? examples/s]Map:  10%|â–ˆ         | 1000/10000 [00:00<00:01, 6831.40 examples/s]Map:  30%|â–ˆâ–ˆâ–ˆ       | 3000/10000 [00:00<00:00, 10956.08 examples/s]Map:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5000/10000 [00:00<00:00, 12427.84 examples/s]Map:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7000/10000 [00:00<00:00, 13079.98 examples/s]Map:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9000/10000 [00:00<00:00, 13255.95 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:00<00:00, 12614.98 examples/s]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 14265.52 examples/s]
wandb: WARNING Metadata is read-only when using pydantic v1.
  0%|          | 0/5000 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
  0%|          | 1/5000 [00:04<6:24:29,  4.61s/it]  0%|          | 2/5000 [00:04<2:44:03,  1.97s/it]  0%|          | 3/5000 [00:04<1:33:23,  1.12s/it]  0%|          | 4/5000 [00:04<1:00:10,  1.38it/s]  0%|          | 5/5000 [00:05<41:47,  1.99it/s]    0%|          | 6/5000 [00:05<30:43,  2.71it/s]  0%|          | 7/5000 [00:05<23:42,  3.51it/s]  0%|          | 8/5000 [00:05<19:05,  4.36it/s]  0%|          | 9/5000 [00:05<16:01,  5.19it/s]  0%|          | 10/5000 [00:05<13:57,  5.96it/s]  0%|          | 11/5000 [00:05<12:32,  6.63it/s]  0%|          | 12/5000 [00:05<11:34,  7.18it/s]  0%|          | 13/5000 [00:05<10:53,  7.63it/s]  0%|          | 14/5000 [00:06<10:25,  7.97it/s]  0%|          | 15/5000 [00:06<10:06,  8.21it/s]  0%|          | 16/5000 [00:06<09:53,  8.39it/s]  0%|          | 17/5000 [00:06<09:44,  8.53it/s]  0%|          | 18/5000 [00:06<09:36,  8.64it/s]  0%|          | 19/5000 [00:06<09:32,  8.71it/s]  0%|          | 20/5000 [00:06<09:28,  8.76it/s]  0%|          | 21/5000 [00:06<09:25,  8.80it/s]  0%|          | 22/5000 [00:06<09:23,  8.83it/s]  0%|          | 23/5000 [00:07<09:22,  8.85it/s]  0%|          | 24/5000 [00:07<09:21,  8.86it/s]  0%|          | 25/5000 [00:07<09:20,  8.88it/s]  1%|          | 26/5000 [00:07<09:20,  8.88it/s]  1%|          | 27/5000 [00:07<09:19,  8.89it/s]  1%|          | 28/5000 [00:07<09:18,  8.90it/s]  1%|          | 29/5000 [00:07<09:18,  8.90it/s]  1%|          | 30/5000 [00:07<09:18,  8.90it/s]  1%|          | 31/5000 [00:07<09:18,  8.90it/s]  1%|          | 32/5000 [00:08<09:18,  8.90it/s]  1%|          | 33/5000 [00:08<09:18,  8.90it/s]  1%|          | 34/5000 [00:08<09:17,  8.90it/s]  1%|          | 35/5000 [00:08<09:17,  8.91it/s]  1%|          | 36/5000 [00:08<09:17,  8.90it/s]  1%|          | 37/5000 [00:08<09:17,  8.90it/s]  1%|          | 38/5000 [00:08<09:16,  8.91it/s]  1%|          | 39/5000 [00:08<09:16,  8.91it/s]  1%|          | 40/5000 [00:08<09:18,  8.88it/s]  1%|          | 41/5000 [00:09<09:17,  8.89it/s]  1%|          | 42/5000 [00:09<09:17,  8.90it/s]  1%|          | 43/5000 [00:09<09:17,  8.90it/s]  1%|          | 44/5000 [00:09<09:16,  8.90it/s]  1%|          | 45/5000 [00:09<09:16,  8.90it/s]  1%|          | 46/5000 [00:09<09:16,  8.91it/s]  1%|          | 47/5000 [00:09<09:15,  8.91it/s]  1%|          | 48/5000 [00:09<09:15,  8.91it/s]  1%|          | 49/5000 [00:10<09:17,  8.89it/s]  1%|          | 50/5000 [00:10<09:16,  8.89it/s]  1%|          | 51/5000 [00:10<09:15,  8.90it/s]  1%|          | 52/5000 [00:10<09:15,  8.91it/s]  1%|          | 53/5000 [00:10<09:17,  8.88it/s]  1%|          | 54/5000 [00:10<09:18,  8.86it/s]  1%|          | 55/5000 [00:10<09:17,  8.87it/s]  1%|          | 56/5000 [00:10<09:16,  8.88it/s]  1%|          | 57/5000 [00:10<09:17,  8.87it/s]  1%|          | 58/5000 [00:11<09:16,  8.88it/s]  1%|          | 59/5000 [00:11<09:16,  8.88it/s]  1%|          | 60/5000 [00:11<09:16,  8.87it/s]  1%|          | 61/5000 [00:11<09:17,  8.86it/s]  1%|          | 62/5000 [00:11<09:17,  8.85it/s]  1%|â–         | 63/5000 [00:11<09:18,  8.85it/s]  1%|â–         | 64/5000 [00:11<09:16,  8.86it/s]  1%|â–         | 65/5000 [00:11<09:17,  8.86it/s]  1%|â–         | 66/5000 [00:11<09:16,  8.87it/s]  1%|â–         | 67/5000 [00:12<09:15,  8.88it/s]  1%|â–         | 68/5000 [00:12<09:16,  8.87it/s]  1%|â–         | 69/5000 [00:12<09:16,  8.86it/s]  1%|â–         | 70/5000 [00:12<09:15,  8.87it/s]  1%|â–         | 71/5000 [00:12<09:14,  8.88it/s]  1%|â–         | 72/5000 [00:12<09:14,  8.88it/s]  1%|â–         | 73/5000 [00:12<09:14,  8.88it/s]  1%|â–         | 74/5000 [00:12<09:15,  8.87it/s]  2%|â–         | 75/5000 [00:12<09:15,  8.87it/s]  2%|â–         | 76/5000 [00:13<09:15,  8.87it/s]  2%|â–         | 77/5000 [00:13<09:14,  8.88it/s]  2%|â–         | 78/5000 [00:13<09:13,  8.89it/s]  2%|â–         | 79/5000 [00:13<09:14,  8.87it/s]  2%|â–         | 80/5000 [00:13<09:15,  8.86it/s]  2%|â–         | 81/5000 [00:13<09:14,  8.87it/s]  2%|â–         | 82/5000 [00:13<09:13,  8.88it/s]  2%|â–         | 83/5000 [00:13<09:14,  8.87it/s]  2%|â–         | 84/5000 [00:13<09:13,  8.88it/s]  2%|â–         | 85/5000 [00:14<09:13,  8.88it/s]  2%|â–         | 86/5000 [00:14<09:13,  8.88it/s]  2%|â–         | 87/5000 [00:14<09:13,  8.87it/s]  2%|â–         | 88/5000 [00:14<09:14,  8.86it/s]  2%|â–         | 89/5000 [00:14<09:14,  8.86it/s]  2%|â–         | 90/5000 [00:14<09:13,  8.87it/s]  2%|â–         | 91/5000 [00:14<09:12,  8.88it/s]  2%|â–         | 92/5000 [00:14<09:13,  8.87it/s]  2%|â–         | 93/5000 [00:14<09:13,  8.87it/s]  2%|â–         | 94/5000 [00:15<09:13,  8.87it/s]  2%|â–         | 95/5000 [00:15<09:12,  8.88it/s]  2%|â–         | 96/5000 [00:15<09:12,  8.87it/s]  2%|â–         | 97/5000 [00:15<09:12,  8.88it/s]  2%|â–         | 98/5000 [00:15<09:11,  8.89it/s]  2%|â–         | 99/5000 [00:15<09:12,  8.87it/s]  2%|â–         | 100/5000 [00:15<09:13,  8.86it/s]                                                    2%|â–         | 100/5000 [00:16<09:13,  8.86it/s]
  0%|          | 0/125 [00:00<?, ?it/s][A
  2%|â–         | 3/125 [00:00<00:05, 22.61it/s][A
  5%|â–         | 6/125 [00:00<00:06, 17.06it/s][A
  6%|â–‹         | 8/125 [00:00<00:07, 16.06it/s][A
  8%|â–Š         | 10/125 [00:00<00:07, 15.53it/s][A
 10%|â–‰         | 12/125 [00:00<00:07, 15.21it/s][A
 11%|â–ˆ         | 14/125 [00:00<00:07, 15.00it/s][A
 13%|â–ˆâ–Ž        | 16/125 [00:01<00:07, 14.88it/s][A
 14%|â–ˆâ–        | 18/125 [00:01<00:07, 14.79it/s][A
 16%|â–ˆâ–Œ        | 20/125 [00:01<00:07, 14.71it/s][A
 18%|â–ˆâ–Š        | 22/125 [00:01<00:07, 14.68it/s][A
 19%|â–ˆâ–‰        | 24/125 [00:01<00:06, 14.67it/s][A
 21%|â–ˆâ–ˆ        | 26/125 [00:01<00:06, 14.64it/s][A
 22%|â–ˆâ–ˆâ–       | 28/125 [00:01<00:06, 14.63it/s][A
 24%|â–ˆâ–ˆâ–       | 30/125 [00:01<00:06, 14.63it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 32/125 [00:02<00:06, 14.62it/s][A
 27%|â–ˆâ–ˆâ–‹       | 34/125 [00:02<00:06, 14.63it/s][A
 29%|â–ˆâ–ˆâ–‰       | 36/125 [00:02<00:06, 14.61it/s][A
 30%|â–ˆâ–ˆâ–ˆ       | 38/125 [00:02<00:05, 14.62it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 40/125 [00:02<00:05, 14.62it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 42/125 [00:02<00:05, 14.61it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 44/125 [00:02<00:05, 14.61it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 46/125 [00:03<00:05, 14.60it/s][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 48/125 [00:03<00:05, 14.61it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 50/125 [00:03<00:05, 14.60it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 52/125 [00:03<00:04, 14.60it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 54/125 [00:03<00:04, 14.59it/s][A
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 56/125 [00:03<00:04, 14.61it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 58/125 [00:03<00:04, 14.61it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 60/125 [00:04<00:04, 14.62it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 62/125 [00:04<00:04, 14.63it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 64/125 [00:04<00:04, 14.63it/s][A
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 66/125 [00:04<00:04, 14.62it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 68/125 [00:04<00:03, 14.60it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 70/125 [00:04<00:03, 14.61it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 72/125 [00:04<00:03, 14.61it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 74/125 [00:04<00:03, 14.60it/s][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 76/125 [00:05<00:03, 14.59it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 78/125 [00:05<00:03, 14.60it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 80/125 [00:05<00:03, 14.58it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 82/125 [00:05<00:02, 14.57it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 84/125 [00:05<00:02, 14.57it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 86/125 [00:05<00:02, 14.57it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 88/125 [00:05<00:02, 14.57it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 90/125 [00:06<00:02, 14.56it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 92/125 [00:06<00:02, 14.57it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 94/125 [00:06<00:02, 14.55it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 96/125 [00:06<00:01, 14.58it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 98/125 [00:06<00:01, 14.60it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 100/125 [00:06<00:01, 14.61it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 102/125 [00:06<00:01, 14.59it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 104/125 [00:07<00:01, 14.59it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 106/125 [00:07<00:01, 14.58it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 108/125 [00:07<00:01, 14.60it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 110/125 [00:07<00:01, 14.59it/s][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 112/125 [00:07<00:00, 14.59it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 114/125 [00:07<00:00, 14.58it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 116/125 [00:07<00:00, 14.59it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 118/125 [00:08<00:00, 14.60it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 120/125 [00:08<00:00, 14.60it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 122/125 [00:08<00:00, 14.59it/s][A
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 124/125 [00:08<00:00, 14.59it/s][A                                                  
                                                 [A  2%|â–         | 100/5000 [00:25<09:13,  8.86it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:08<00:00, 14.59it/s][A
                                                 [ATraceback (most recent call last):
  File "/fred/oz413/ANeurIPS2024_SPV-MIA/./ft_llms/llms_finetune.py", line 270, in <module>
    trainer.train()
  File "/fred/oz413/.conda/envs/llm-env/lib/python3.10/site-packages/transformers/trainer.py", line 1591, in train
    return inner_training_loop(
  File "/fred/oz413/.conda/envs/llm-env/lib/python3.10/site-packages/transformers/trainer.py", line 1984, in _inner_training_loop
    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)
  File "/fred/oz413/.conda/envs/llm-env/lib/python3.10/site-packages/transformers/trainer.py", line 2339, in _maybe_log_save_evaluate
    self._save_checkpoint(model, trial, metrics=metrics)
  File "/fred/oz413/.conda/envs/llm-env/lib/python3.10/site-packages/transformers/trainer.py", line 2396, in _save_checkpoint
    self.save_model(output_dir, _internal_call=True)
  File "/fred/oz413/.conda/envs/llm-env/lib/python3.10/site-packages/transformers/trainer.py", line 2901, in save_model
    self._save(output_dir)
  File "/fred/oz413/.conda/envs/llm-env/lib/python3.10/site-packages/transformers/trainer.py", line 2959, in _save
    self.model.save_pretrained(
  File "/fred/oz413/.conda/envs/llm-env/lib/python3.10/site-packages/peft/peft_model.py", line 162, in save_pretrained
    self.create_or_update_model_card(save_directory)
  File "/fred/oz413/.conda/envs/llm-env/lib/python3.10/site-packages/peft/peft_model.py", line 630, in create_or_update_model_card
    add_library_to_model_card(output_dir)
  File "/fred/oz413/.conda/envs/llm-env/lib/python3.10/site-packages/peft/utils/other.py", line 63, in add_library_to_model_card
    with open(os.path.join(output_dir, "README.md"), "w") as f:
PermissionError: [Errno 13] Permission denied: './ft_llms/gpt2-xl/ag_news/baseline/checkpoint-100/README.md'
Traceback (most recent call last):
  File "/fred/oz413/.conda/envs/llm-env/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/fred/oz413/.conda/envs/llm-env/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 47, in main
    args.func(args)
  File "/fred/oz413/.conda/envs/llm-env/lib/python3.10/site-packages/accelerate/commands/launch.py", line 986, in launch_command
    simple_launcher(args)
  File "/fred/oz413/.conda/envs/llm-env/lib/python3.10/site-packages/accelerate/commands/launch.py", line 628, in simple_launcher
    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)
subprocess.CalledProcessError: Command '['/fred/oz413/.conda/envs/llm-env/bin/python', './ft_llms/llms_finetune.py', '--output_dir', './ft_llms/gpt2-xl/ag_news/baseline/', '--block_size', '128', '--eval_steps', '100', '--save_epochs', '100', '--log_steps', '100', '-d', 'ag_news', '-m', 'gpt2-xl', '--packing', '--use_dataset_cache', '-e', '2', '-b', '4', '-lr', '5e-5', '--gradient_accumulation_steps', '1', '--train_sta_idx=0', '--train_end_idx=10000', '--eval_sta_idx=0', '--eval_end_idx=1000', '--dataset_config_name', 'default']' returned non-zero exit status 1.
