Job Started: Thu May 22 21:57:38 AEST 2025
Job ID: 537702
Running on node(s): gina15
Allocated CPUs: 8
Allocated Memory: 65536 MB
Allocated GPU(s) Raw: 0 (using GRES: )
CUDA_VISIBLE_DEVICES: 0
Attempting to load system Mamba module...
Conda (mamba) command found at: conda ()
{ 
    \local cmd="${1-__missing__}";
    case "$cmd" in 
        activate | deactivate)
            __conda_activate "$@"
        ;;
        install | update | upgrade | remove | uninstall)
            __conda_exe "$@" || \return;
            __conda_reactivate
        ;;
        *)
            __conda_exe "$@"
        ;;
    esac
}
Activating Conda env: llm-env
Python location: /fred/oz413/.conda/envs/llm-env/bin/python
Python version: Python 3.10.16
PyTorch CUDA available: True
Number of GPUs PyTorch sees: 1
Thu May 22 21:59:07 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.124.06             Driver Version: 570.124.06     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-80GB          On  |   00000000:01:00.0 Off |                    0 |
| N/A   33C    P0             65W /  500W |       1MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Working directory: /fred/oz413/ANeurIPS2024_SPV-MIA
Launching training with Accelerate...
[05/22/25 22:02:44] INFO     Model is not llama, disabling  llms_finetune.py:102
                             flash attention...                                 
[05/22/25 22:02:45] INFO     Pad token id is None, setting  llms_finetune.py:136
                             to eos token id...                                 
                    INFO     Using a block size of 128      llms_finetune.py:140
                    INFO     Using no quantization          llms_finetune.py:158
[05/22/25 22:03:09] INFO     Using PEFT...                  llms_finetune.py:199
                    INFO     Getting PEFT model...          llms_finetune.py:207
trainable params: 19660800 || all params: 1577272000 || trainable%: 1.2465066266312976
DEBUG: args object in dataset_prepare: Namespace(model_name='gpt2-xl', dataset_name='ag_news', dataset_config_name='default', cache_path='./cache', use_dataset_cache=True, refer=False, refer_data_source=None, packing=True, token=None, split_model=False, block_size=128, preprocessing_num_workers=1, peft='lora', lora_rank=64, lora_alpha=16, lora_dropout=0.1, p_tokens=20, p_hidden=128, learning_rate=5e-05, lr_scheduler_type='linear', warmup_steps=0, weight_decay=0, output_dir='./ft_llms/gpt2-xl/ag_news/baseline/', log_steps=100, eval_steps=100, save_epochs=100, epochs=2, batch_size=4, gradient_accumulation_steps=1, gradient_checkpointing=False, trust_remote_code=False, train_sta_idx=0, train_end_idx=10000, eval_sta_idx=0, eval_end_idx=1000, save_limit=None, use_int4=False, use_int8=False, disable_peft=False, disable_flash_attention=False, pad_token_id=None, add_eos_token=False, add_bos_token=False, validation_split_percentage=0.1)
Found dataset at specific config path: /fred/oz413/LLM/ag_news/default
Loading dataset from disk: /fred/oz413/LLM/ag_news/default
Identified text column as: 'text'
Applying text packing...
Folder './cache/ag_news/default' already exists.
Folder './cache/ag_news/default' already exists.
[05/22/25 22:03:14] INFO     Training with 1 GPUs           llms_finetune.py:228
{'loss': 3.7255, 'learning_rate': 4.9e-05, 'epoch': 0.04}
{'eval_loss': 3.5710136890411377, 'eval_runtime': 8.5961, 'eval_samples_per_second': 116.332, 'eval_steps_per_second': 14.542, 'epoch': 0.04}
{'loss': 3.5437, 'learning_rate': 4.8e-05, 'epoch': 0.08}
{'eval_loss': 3.4111649990081787, 'eval_runtime': 8.5908, 'eval_samples_per_second': 116.404, 'eval_steps_per_second': 14.55, 'epoch': 0.08}
{'loss': 3.3976, 'learning_rate': 4.7e-05, 'epoch': 0.12}
{'eval_loss': 3.31341290473938, 'eval_runtime': 8.5913, 'eval_samples_per_second': 116.396, 'eval_steps_per_second': 14.55, 'epoch': 0.12}
{'loss': 3.365, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.16}
{'eval_loss': 3.2580277919769287, 'eval_runtime': 8.6102, 'eval_samples_per_second': 116.142, 'eval_steps_per_second': 14.518, 'epoch': 0.16}
{'loss': 3.3034, 'learning_rate': 4.5e-05, 'epoch': 0.2}
{'eval_loss': 3.221890926361084, 'eval_runtime': 8.5854, 'eval_samples_per_second': 116.477, 'eval_steps_per_second': 14.56, 'epoch': 0.2}
{'loss': 3.2878, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.24}
{'eval_loss': 3.1939291954040527, 'eval_runtime': 8.5901, 'eval_samples_per_second': 116.413, 'eval_steps_per_second': 14.552, 'epoch': 0.24}
{'loss': 3.2711, 'learning_rate': 4.3e-05, 'epoch': 0.28}
{'eval_loss': 3.173389196395874, 'eval_runtime': 8.5843, 'eval_samples_per_second': 116.491, 'eval_steps_per_second': 14.561, 'epoch': 0.28}
{'loss': 3.2199, 'learning_rate': 4.2e-05, 'epoch': 0.32}
{'eval_loss': 3.1543798446655273, 'eval_runtime': 8.5879, 'eval_samples_per_second': 116.443, 'eval_steps_per_second': 14.555, 'epoch': 0.32}
{'loss': 3.2568, 'learning_rate': 4.1e-05, 'epoch': 0.36}
{'eval_loss': 3.14213228225708, 'eval_runtime': 8.585, 'eval_samples_per_second': 116.482, 'eval_steps_per_second': 14.56, 'epoch': 0.36}
{'loss': 3.2369, 'learning_rate': 4e-05, 'epoch': 0.4}
{'eval_loss': 3.131673574447632, 'eval_runtime': 8.6019, 'eval_samples_per_second': 116.253, 'eval_steps_per_second': 14.532, 'epoch': 0.4}
{'loss': 3.1842, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.44}
{'eval_loss': 3.1189324855804443, 'eval_runtime': 8.594, 'eval_samples_per_second': 116.36, 'eval_steps_per_second': 14.545, 'epoch': 0.44}
{'loss': 3.1904, 'learning_rate': 3.8e-05, 'epoch': 0.48}
{'eval_loss': 3.1094350814819336, 'eval_runtime': 8.5965, 'eval_samples_per_second': 116.326, 'eval_steps_per_second': 14.541, 'epoch': 0.48}
{'loss': 3.2306, 'learning_rate': 3.7e-05, 'epoch': 0.52}
{'eval_loss': 3.102621555328369, 'eval_runtime': 8.6186, 'eval_samples_per_second': 116.028, 'eval_steps_per_second': 14.503, 'epoch': 0.52}
{'loss': 3.1872, 'learning_rate': 3.6e-05, 'epoch': 0.56}
{'eval_loss': 3.0948500633239746, 'eval_runtime': 8.6208, 'eval_samples_per_second': 115.998, 'eval_steps_per_second': 14.5, 'epoch': 0.56}
{'loss': 3.1378, 'learning_rate': 3.5e-05, 'epoch': 0.6}
{'eval_loss': 3.0894672870635986, 'eval_runtime': 8.6295, 'eval_samples_per_second': 115.881, 'eval_steps_per_second': 14.485, 'epoch': 0.6}
{'loss': 3.1808, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.64}
{'eval_loss': 3.083889961242676, 'eval_runtime': 8.6095, 'eval_samples_per_second': 116.151, 'eval_steps_per_second': 14.519, 'epoch': 0.64}
{'loss': 3.1379, 'learning_rate': 3.3e-05, 'epoch': 0.68}
{'eval_loss': 3.0804998874664307, 'eval_runtime': 8.6274, 'eval_samples_per_second': 115.91, 'eval_steps_per_second': 14.489, 'epoch': 0.68}
{'loss': 3.1472, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.72}
{'eval_loss': 3.0722551345825195, 'eval_runtime': 8.5928, 'eval_samples_per_second': 116.377, 'eval_steps_per_second': 14.547, 'epoch': 0.72}
{'loss': 3.1162, 'learning_rate': 3.1e-05, 'epoch': 0.76}
{'eval_loss': 3.0684428215026855, 'eval_runtime': 8.591, 'eval_samples_per_second': 116.401, 'eval_steps_per_second': 14.55, 'epoch': 0.76}
{'loss': 3.1504, 'learning_rate': 3e-05, 'epoch': 0.8}
{'eval_loss': 3.0653562545776367, 'eval_runtime': 8.6297, 'eval_samples_per_second': 115.879, 'eval_steps_per_second': 14.485, 'epoch': 0.8}
{'loss': 3.1509, 'learning_rate': 2.9e-05, 'epoch': 0.84}
{'eval_loss': 3.059324264526367, 'eval_runtime': 8.5837, 'eval_samples_per_second': 116.5, 'eval_steps_per_second': 14.563, 'epoch': 0.84}
{'loss': 3.1407, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.88}
{'eval_loss': 3.055234432220459, 'eval_runtime': 8.595, 'eval_samples_per_second': 116.347, 'eval_steps_per_second': 14.543, 'epoch': 0.88}
{'loss': 3.1029, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.92}
{'eval_loss': 3.0525457859039307, 'eval_runtime': 8.5776, 'eval_samples_per_second': 116.582, 'eval_steps_per_second': 14.573, 'epoch': 0.92}
{'loss': 3.125, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.96}
{'eval_loss': 3.050393581390381, 'eval_runtime': 8.6008, 'eval_samples_per_second': 116.268, 'eval_steps_per_second': 14.533, 'epoch': 0.96}
{'loss': 3.1085, 'learning_rate': 2.5e-05, 'epoch': 1.0}
{'eval_loss': 3.048286199569702, 'eval_runtime': 8.6274, 'eval_samples_per_second': 115.909, 'eval_steps_per_second': 14.489, 'epoch': 1.0}
{'loss': 3.1135, 'learning_rate': 2.4e-05, 'epoch': 1.04}
{'eval_loss': 3.0448238849639893, 'eval_runtime': 8.5966, 'eval_samples_per_second': 116.325, 'eval_steps_per_second': 14.541, 'epoch': 1.04}
{'loss': 3.1156, 'learning_rate': 2.3000000000000003e-05, 'epoch': 1.08}
{'eval_loss': 3.044058084487915, 'eval_runtime': 8.5958, 'eval_samples_per_second': 116.336, 'eval_steps_per_second': 14.542, 'epoch': 1.08}
{'loss': 3.1232, 'learning_rate': 2.2000000000000003e-05, 'epoch': 1.12}
{'eval_loss': 3.0401036739349365, 'eval_runtime': 8.5795, 'eval_samples_per_second': 116.557, 'eval_steps_per_second': 14.57, 'epoch': 1.12}
{'loss': 3.1293, 'learning_rate': 2.1e-05, 'epoch': 1.16}
{'eval_loss': 3.039045810699463, 'eval_runtime': 8.615, 'eval_samples_per_second': 116.076, 'eval_steps_per_second': 14.51, 'epoch': 1.16}
{'loss': 3.1144, 'learning_rate': 2e-05, 'epoch': 1.2}
{'eval_loss': 3.037174940109253, 'eval_runtime': 8.5948, 'eval_samples_per_second': 116.35, 'eval_steps_per_second': 14.544, 'epoch': 1.2}
{'loss': 3.0878, 'learning_rate': 1.9e-05, 'epoch': 1.24}
{'eval_loss': 3.036144733428955, 'eval_runtime': 8.5921, 'eval_samples_per_second': 116.386, 'eval_steps_per_second': 14.548, 'epoch': 1.24}
{'loss': 3.1027, 'learning_rate': 1.8e-05, 'epoch': 1.28}
{'eval_loss': 3.03452205657959, 'eval_runtime': 8.6361, 'eval_samples_per_second': 115.793, 'eval_steps_per_second': 14.474, 'epoch': 1.28}
{'loss': 3.0611, 'learning_rate': 1.7000000000000003e-05, 'epoch': 1.32}
{'eval_loss': 3.033017158508301, 'eval_runtime': 8.5935, 'eval_samples_per_second': 116.367, 'eval_steps_per_second': 14.546, 'epoch': 1.32}
{'loss': 3.0709, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.36}
{'eval_loss': 3.032430410385132, 'eval_runtime': 8.6281, 'eval_samples_per_second': 115.9, 'eval_steps_per_second': 14.487, 'epoch': 1.36}
{'loss': 3.1189, 'learning_rate': 1.5e-05, 'epoch': 1.4}
{'eval_loss': 3.031238555908203, 'eval_runtime': 8.6045, 'eval_samples_per_second': 116.218, 'eval_steps_per_second': 14.527, 'epoch': 1.4}
{'loss': 3.0334, 'learning_rate': 1.4000000000000001e-05, 'epoch': 1.44}
{'eval_loss': 3.0299482345581055, 'eval_runtime': 8.6187, 'eval_samples_per_second': 116.026, 'eval_steps_per_second': 14.503, 'epoch': 1.44}
{'loss': 3.0425, 'learning_rate': 1.3000000000000001e-05, 'epoch': 1.48}
{'eval_loss': 3.0284183025360107, 'eval_runtime': 8.619, 'eval_samples_per_second': 116.023, 'eval_steps_per_second': 14.503, 'epoch': 1.48}
{'loss': 3.098, 'learning_rate': 1.2e-05, 'epoch': 1.52}
{'eval_loss': 3.0270776748657227, 'eval_runtime': 8.6199, 'eval_samples_per_second': 116.01, 'eval_steps_per_second': 14.501, 'epoch': 1.52}
{'loss': 3.1311, 'learning_rate': 1.1000000000000001e-05, 'epoch': 1.56}
{'eval_loss': 3.025195837020874, 'eval_runtime': 8.5935, 'eval_samples_per_second': 116.367, 'eval_steps_per_second': 14.546, 'epoch': 1.56}
{'loss': 3.0586, 'learning_rate': 1e-05, 'epoch': 1.6}
{'eval_loss': 3.0247349739074707, 'eval_runtime': 8.5943, 'eval_samples_per_second': 116.357, 'eval_steps_per_second': 14.545, 'epoch': 1.6}
{'loss': 3.1131, 'learning_rate': 9e-06, 'epoch': 1.64}
{'eval_loss': 3.023336887359619, 'eval_runtime': 8.6164, 'eval_samples_per_second': 116.057, 'eval_steps_per_second': 14.507, 'epoch': 1.64}
{'loss': 3.0235, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.68}
{'eval_loss': 3.0232746601104736, 'eval_runtime': 8.6289, 'eval_samples_per_second': 115.889, 'eval_steps_per_second': 14.486, 'epoch': 1.68}
{'loss': 3.0729, 'learning_rate': 7.000000000000001e-06, 'epoch': 1.72}
{'eval_loss': 3.0225796699523926, 'eval_runtime': 8.5826, 'eval_samples_per_second': 116.515, 'eval_steps_per_second': 14.564, 'epoch': 1.72}
{'loss': 3.0604, 'learning_rate': 6e-06, 'epoch': 1.76}
{'eval_loss': 3.022437334060669, 'eval_runtime': 8.644, 'eval_samples_per_second': 115.688, 'eval_steps_per_second': 14.461, 'epoch': 1.76}
{'loss': 3.0686, 'learning_rate': 5e-06, 'epoch': 1.8}
{'eval_loss': 3.021700143814087, 'eval_runtime': 8.6021, 'eval_samples_per_second': 116.251, 'eval_steps_per_second': 14.531, 'epoch': 1.8}
{'loss': 3.1047, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.84}
{'eval_loss': 3.021315813064575, 'eval_runtime': 8.5942, 'eval_samples_per_second': 116.357, 'eval_steps_per_second': 14.545, 'epoch': 1.84}
{'loss': 3.0435, 'learning_rate': 3e-06, 'epoch': 1.88}
{'eval_loss': 3.0208139419555664, 'eval_runtime': 8.6147, 'eval_samples_per_second': 116.08, 'eval_steps_per_second': 14.51, 'epoch': 1.88}
{'loss': 3.0961, 'learning_rate': 2.0000000000000003e-06, 'epoch': 1.92}
{'eval_loss': 3.020867347717285, 'eval_runtime': 8.623, 'eval_samples_per_second': 115.969, 'eval_steps_per_second': 14.496, 'epoch': 1.92}
{'loss': 3.0874, 'learning_rate': 1.0000000000000002e-06, 'epoch': 1.96}
{'eval_loss': 3.0207877159118652, 'eval_runtime': 8.6053, 'eval_samples_per_second': 116.207, 'eval_steps_per_second': 14.526, 'epoch': 1.96}
{'loss': 3.0976, 'learning_rate': 0.0, 'epoch': 2.0}
{'eval_loss': 3.020566940307617, 'eval_runtime': 8.6075, 'eval_samples_per_second': 116.178, 'eval_steps_per_second': 14.522, 'epoch': 2.0}
{'train_runtime': 1091.5871, 'train_samples_per_second': 18.322, 'train_steps_per_second': 4.58, 'train_loss': 3.161345489501953, 'epoch': 2.0}
Fine-tuning completed successfully.
Job Ended: Thu May 22 22:22:16 AEST 2025

+------------------- Job Report: 537702 (COMPLETED) -------------------+
| Memory (RAM)  [##                  ] 10.2% (6.5 GB peak / 64 GB)     |
| CPU           [#                   ]  8.9% average                   |
| GPU           [##########          ] 53.3% average                   |
| Time          [-------->           ] 41.5% (0-00:24:55 / 0-01:00:00) |
|                                                                      |
| Lustre Filesystem:                                                   |
|   Path    Total Read    Total Write    Total IOPS                    |
|   ------  ------------  -------------  ------------                  |
|   /apps   24.9 MB       0 B            1.7 K                         |
|   /fred   506.8 MB      10.9 GB        28 K                          |
|   /home   8 KB          0 B            15                            |
|   OS      0 B           0 B            19                            |
|                                                                      |
| Warnings:                                                            |
|   - Too much memory requested                                        |
|   - CPU usage is low                                                 |
|   - GPU usage is low                                                 |
|   - Too much time requested                                          |
+----------------------------------------------------------------------+
