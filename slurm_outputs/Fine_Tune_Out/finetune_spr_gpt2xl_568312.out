Job Started: Fri May 23 15:41:53 AEST 2025
Job ID: 568312
Running on node(s): gina6
Allocated CPUs: 8
Allocated Memory: 65536 MB
Allocated GPU(s) Raw: 2 (using GRES: )
CUDA_VISIBLE_DEVICES: 0
Attempting to load system Mamba module...
Conda (mamba) command found at: conda ()
{ 
    \local cmd="${1-__missing__}";
    case "$cmd" in 
        activate | deactivate)
            __conda_activate "$@"
        ;;
        install | update | upgrade | remove | uninstall)
            __conda_exe "$@" || \return;
            __conda_reactivate
        ;;
        *)
            __conda_exe "$@"
        ;;
    esac
}
Activating Conda env: llm-env
Python location: /fred/oz413/.conda/envs/llm-env/bin/python
Python version: Python 3.10.16
PyTorch CUDA available: True
Number of GPUs PyTorch sees: 1
Fri May 23 15:42:05 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.124.06             Driver Version: 570.124.06     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-80GB          On  |   00000000:81:00.0 Off |                    0 |
| N/A   40C    P0             66W /  500W |       1MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Working directory: /fred/oz413/ANeurIPS2024_SPV-MIA
Launching training with Accelerate...
[05/23/25 15:42:23] INFO     Model is not llama, disabling  llms_finetune.py:102
                             flash attention...                                 
                    INFO     Pad token id is None, setting  llms_finetune.py:136
                             to eos token id...                                 
                    INFO     Using a block size of 128      llms_finetune.py:140
                    INFO     Using no quantization          llms_finetune.py:158
[05/23/25 15:42:45] INFO     Using PEFT...                  llms_finetune.py:199
                    INFO     Getting PEFT model...          llms_finetune.py:207
trainable params: 19660800 || all params: 1577272000 || trainable%: 1.2465066266312976
DEBUG: args object in dataset_prepare: Namespace(model_name='gpt2-xl', dataset_name='ag_news', dataset_config_name='default', cache_path='./cache', use_dataset_cache=True, refer=False, refer_data_source=None, packing=True, token=None, split_model=False, block_size=128, preprocessing_num_workers=1, peft='lora', lora_rank=64, lora_alpha=16, lora_dropout=0.1, p_tokens=20, p_hidden=128, learning_rate=5e-05, lr_scheduler_type='linear', warmup_steps=0, weight_decay=0, output_dir='./ft_llms/gpt2-xl/ag_news/baseline/', log_steps=100, eval_steps=100, save_epochs=100, epochs=2, batch_size=4, gradient_accumulation_steps=1, gradient_checkpointing=False, trust_remote_code=False, train_sta_idx=0, train_end_idx=10000, eval_sta_idx=0, eval_end_idx=1000, save_limit=None, use_int4=False, use_int8=False, disable_peft=False, disable_flash_attention=False, pad_token_id=None, add_eos_token=False, add_bos_token=False, validation_split_percentage=0.1)
Found dataset at specific config path: /fred/oz413/LLM/ag_news/default
Loading dataset from disk: /fred/oz413/LLM/ag_news/default
Identified text column as: 'text'
Applying text packing...
Folder './cache/ag_news/default' already exists.
Folder './cache/ag_news/default' already exists.
[05/23/25 15:42:49] INFO     Training with 1 GPUs           llms_finetune.py:228
{'loss': 3.7244, 'learning_rate': 4.9e-05, 'epoch': 0.04}
{'eval_loss': 3.5702576637268066, 'eval_runtime': 8.9361, 'eval_samples_per_second': 111.906, 'eval_steps_per_second': 13.988, 'epoch': 0.04}
{'loss': 3.5432, 'learning_rate': 4.8e-05, 'epoch': 0.08}
{'eval_loss': 3.410646438598633, 'eval_runtime': 8.9317, 'eval_samples_per_second': 111.961, 'eval_steps_per_second': 13.995, 'epoch': 0.08}
{'loss': 3.3975, 'learning_rate': 4.7e-05, 'epoch': 0.12}
{'eval_loss': 3.313237190246582, 'eval_runtime': 8.9455, 'eval_samples_per_second': 111.788, 'eval_steps_per_second': 13.974, 'epoch': 0.12}
{'loss': 3.3649, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.16}
{'eval_loss': 3.258059024810791, 'eval_runtime': 8.9253, 'eval_samples_per_second': 112.042, 'eval_steps_per_second': 14.005, 'epoch': 0.16}
{'loss': 3.3036, 'learning_rate': 4.5e-05, 'epoch': 0.2}
{'eval_loss': 3.2223193645477295, 'eval_runtime': 8.955, 'eval_samples_per_second': 111.67, 'eval_steps_per_second': 13.959, 'epoch': 0.2}
{'loss': 3.2881, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.24}
{'eval_loss': 3.194580078125, 'eval_runtime': 8.9488, 'eval_samples_per_second': 111.747, 'eval_steps_per_second': 13.968, 'epoch': 0.24}
{'loss': 3.2717, 'learning_rate': 4.3e-05, 'epoch': 0.28}
{'eval_loss': 3.1740965843200684, 'eval_runtime': 8.936, 'eval_samples_per_second': 111.907, 'eval_steps_per_second': 13.988, 'epoch': 0.28}
{'loss': 3.2207, 'learning_rate': 4.2e-05, 'epoch': 0.32}
{'eval_loss': 3.1550021171569824, 'eval_runtime': 8.9578, 'eval_samples_per_second': 111.635, 'eval_steps_per_second': 13.954, 'epoch': 0.32}
{'loss': 3.2576, 'learning_rate': 4.1e-05, 'epoch': 0.36}
{'eval_loss': 3.1426889896392822, 'eval_runtime': 8.949, 'eval_samples_per_second': 111.745, 'eval_steps_per_second': 13.968, 'epoch': 0.36}
{'loss': 3.2374, 'learning_rate': 4e-05, 'epoch': 0.4}
{'eval_loss': 3.1323494911193848, 'eval_runtime': 8.9416, 'eval_samples_per_second': 111.837, 'eval_steps_per_second': 13.98, 'epoch': 0.4}
{'loss': 3.1846, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.44}
{'eval_loss': 3.119720697402954, 'eval_runtime': 8.9438, 'eval_samples_per_second': 111.809, 'eval_steps_per_second': 13.976, 'epoch': 0.44}
{'loss': 3.1911, 'learning_rate': 3.8e-05, 'epoch': 0.48}
{'eval_loss': 3.1103343963623047, 'eval_runtime': 8.9397, 'eval_samples_per_second': 111.861, 'eval_steps_per_second': 13.983, 'epoch': 0.48}
{'loss': 3.2311, 'learning_rate': 3.7e-05, 'epoch': 0.52}
{'eval_loss': 3.1030893325805664, 'eval_runtime': 8.9501, 'eval_samples_per_second': 111.731, 'eval_steps_per_second': 13.966, 'epoch': 0.52}
{'loss': 3.1877, 'learning_rate': 3.6e-05, 'epoch': 0.56}
{'eval_loss': 3.095248222351074, 'eval_runtime': 8.9601, 'eval_samples_per_second': 111.605, 'eval_steps_per_second': 13.951, 'epoch': 0.56}
{'loss': 3.1389, 'learning_rate': 3.5e-05, 'epoch': 0.6}
{'eval_loss': 3.0898427963256836, 'eval_runtime': 8.9306, 'eval_samples_per_second': 111.974, 'eval_steps_per_second': 13.997, 'epoch': 0.6}
{'loss': 3.181, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.64}
{'eval_loss': 3.0843753814697266, 'eval_runtime': 8.9322, 'eval_samples_per_second': 111.955, 'eval_steps_per_second': 13.994, 'epoch': 0.64}
{'loss': 3.138, 'learning_rate': 3.3e-05, 'epoch': 0.68}
{'eval_loss': 3.0808486938476562, 'eval_runtime': 8.9296, 'eval_samples_per_second': 111.987, 'eval_steps_per_second': 13.998, 'epoch': 0.68}
{'loss': 3.1476, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.72}
{'eval_loss': 3.072505474090576, 'eval_runtime': 8.9382, 'eval_samples_per_second': 111.879, 'eval_steps_per_second': 13.985, 'epoch': 0.72}
{'loss': 3.1162, 'learning_rate': 3.1e-05, 'epoch': 0.76}
{'eval_loss': 3.068709373474121, 'eval_runtime': 8.9312, 'eval_samples_per_second': 111.966, 'eval_steps_per_second': 13.996, 'epoch': 0.76}
{'loss': 3.1508, 'learning_rate': 3e-05, 'epoch': 0.8}
{'eval_loss': 3.065505266189575, 'eval_runtime': 8.9353, 'eval_samples_per_second': 111.916, 'eval_steps_per_second': 13.989, 'epoch': 0.8}
{'loss': 3.1517, 'learning_rate': 2.9e-05, 'epoch': 0.84}
{'eval_loss': 3.059683084487915, 'eval_runtime': 8.9463, 'eval_samples_per_second': 111.779, 'eval_steps_per_second': 13.972, 'epoch': 0.84}
{'loss': 3.1408, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.88}
{'eval_loss': 3.055426836013794, 'eval_runtime': 8.9283, 'eval_samples_per_second': 112.003, 'eval_steps_per_second': 14.0, 'epoch': 0.88}
{'loss': 3.1032, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.92}
{'eval_loss': 3.0528271198272705, 'eval_runtime': 8.9317, 'eval_samples_per_second': 111.961, 'eval_steps_per_second': 13.995, 'epoch': 0.92}
{'loss': 3.1256, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.96}
{'eval_loss': 3.050816297531128, 'eval_runtime': 8.9375, 'eval_samples_per_second': 111.889, 'eval_steps_per_second': 13.986, 'epoch': 0.96}
{'loss': 3.1085, 'learning_rate': 2.5e-05, 'epoch': 1.0}
{'eval_loss': 3.0483834743499756, 'eval_runtime': 8.933, 'eval_samples_per_second': 111.945, 'eval_steps_per_second': 13.993, 'epoch': 1.0}
{'loss': 3.1141, 'learning_rate': 2.4e-05, 'epoch': 1.04}
{'eval_loss': 3.0450780391693115, 'eval_runtime': 8.9488, 'eval_samples_per_second': 111.746, 'eval_steps_per_second': 13.968, 'epoch': 1.04}
{'loss': 3.1158, 'learning_rate': 2.3000000000000003e-05, 'epoch': 1.08}
{'eval_loss': 3.0442705154418945, 'eval_runtime': 8.9444, 'eval_samples_per_second': 111.801, 'eval_steps_per_second': 13.975, 'epoch': 1.08}
{'loss': 3.1239, 'learning_rate': 2.2000000000000003e-05, 'epoch': 1.12}
{'eval_loss': 3.0401453971862793, 'eval_runtime': 8.9399, 'eval_samples_per_second': 111.858, 'eval_steps_per_second': 13.982, 'epoch': 1.12}
{'loss': 3.1295, 'learning_rate': 2.1e-05, 'epoch': 1.16}
{'eval_loss': 3.0392324924468994, 'eval_runtime': 8.9106, 'eval_samples_per_second': 112.226, 'eval_steps_per_second': 14.028, 'epoch': 1.16}
{'loss': 3.1146, 'learning_rate': 2e-05, 'epoch': 1.2}
{'eval_loss': 3.0374207496643066, 'eval_runtime': 8.9267, 'eval_samples_per_second': 112.023, 'eval_steps_per_second': 14.003, 'epoch': 1.2}
{'loss': 3.0884, 'learning_rate': 1.9e-05, 'epoch': 1.24}
{'eval_loss': 3.0364112854003906, 'eval_runtime': 8.8423, 'eval_samples_per_second': 113.093, 'eval_steps_per_second': 14.137, 'epoch': 1.24}
{'loss': 3.103, 'learning_rate': 1.8e-05, 'epoch': 1.28}
{'eval_loss': 3.0346736907958984, 'eval_runtime': 8.8584, 'eval_samples_per_second': 112.888, 'eval_steps_per_second': 14.111, 'epoch': 1.28}
{'loss': 3.0614, 'learning_rate': 1.7000000000000003e-05, 'epoch': 1.32}
{'eval_loss': 3.0333199501037598, 'eval_runtime': 8.85, 'eval_samples_per_second': 112.995, 'eval_steps_per_second': 14.124, 'epoch': 1.32}
{'loss': 3.0711, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.36}
{'eval_loss': 3.032484769821167, 'eval_runtime': 8.8679, 'eval_samples_per_second': 112.766, 'eval_steps_per_second': 14.096, 'epoch': 1.36}
{'loss': 3.1192, 'learning_rate': 1.5e-05, 'epoch': 1.4}
{'eval_loss': 3.031358480453491, 'eval_runtime': 8.8298, 'eval_samples_per_second': 113.253, 'eval_steps_per_second': 14.157, 'epoch': 1.4}
{'loss': 3.0336, 'learning_rate': 1.4000000000000001e-05, 'epoch': 1.44}
{'eval_loss': 3.0302844047546387, 'eval_runtime': 8.8648, 'eval_samples_per_second': 112.806, 'eval_steps_per_second': 14.101, 'epoch': 1.44}
{'loss': 3.0425, 'learning_rate': 1.3000000000000001e-05, 'epoch': 1.48}
{'eval_loss': 3.028763771057129, 'eval_runtime': 8.8448, 'eval_samples_per_second': 113.061, 'eval_steps_per_second': 14.133, 'epoch': 1.48}
{'loss': 3.0984, 'learning_rate': 1.2e-05, 'epoch': 1.52}
{'eval_loss': 3.027360439300537, 'eval_runtime': 8.8186, 'eval_samples_per_second': 113.397, 'eval_steps_per_second': 14.175, 'epoch': 1.52}
{'loss': 3.1313, 'learning_rate': 1.1000000000000001e-05, 'epoch': 1.56}
{'eval_loss': 3.025418758392334, 'eval_runtime': 8.8341, 'eval_samples_per_second': 113.198, 'eval_steps_per_second': 14.15, 'epoch': 1.56}
{'loss': 3.0589, 'learning_rate': 1e-05, 'epoch': 1.6}
{'eval_loss': 3.0250449180603027, 'eval_runtime': 8.8323, 'eval_samples_per_second': 113.221, 'eval_steps_per_second': 14.153, 'epoch': 1.6}
{'loss': 3.1134, 'learning_rate': 9e-06, 'epoch': 1.64}
{'eval_loss': 3.02353572845459, 'eval_runtime': 8.8242, 'eval_samples_per_second': 113.325, 'eval_steps_per_second': 14.166, 'epoch': 1.64}
{'loss': 3.0243, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.68}
{'eval_loss': 3.0235469341278076, 'eval_runtime': 8.8313, 'eval_samples_per_second': 113.233, 'eval_steps_per_second': 14.154, 'epoch': 1.68}
{'loss': 3.0734, 'learning_rate': 7.000000000000001e-06, 'epoch': 1.72}
{'eval_loss': 3.0227437019348145, 'eval_runtime': 8.815, 'eval_samples_per_second': 113.443, 'eval_steps_per_second': 14.18, 'epoch': 1.72}
{'loss': 3.0605, 'learning_rate': 6e-06, 'epoch': 1.76}
{'eval_loss': 3.0225486755371094, 'eval_runtime': 8.8008, 'eval_samples_per_second': 113.626, 'eval_steps_per_second': 14.203, 'epoch': 1.76}
{'loss': 3.0687, 'learning_rate': 5e-06, 'epoch': 1.8}
{'eval_loss': 3.021960973739624, 'eval_runtime': 8.7878, 'eval_samples_per_second': 113.794, 'eval_steps_per_second': 14.224, 'epoch': 1.8}
{'loss': 3.105, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.84}
{'eval_loss': 3.021555185317993, 'eval_runtime': 8.8162, 'eval_samples_per_second': 113.428, 'eval_steps_per_second': 14.178, 'epoch': 1.84}
{'loss': 3.0436, 'learning_rate': 3e-06, 'epoch': 1.88}
{'eval_loss': 3.021052360534668, 'eval_runtime': 8.9568, 'eval_samples_per_second': 111.647, 'eval_steps_per_second': 13.956, 'epoch': 1.88}
{'loss': 3.0965, 'learning_rate': 2.0000000000000003e-06, 'epoch': 1.92}
{'eval_loss': 3.0211126804351807, 'eval_runtime': 8.962, 'eval_samples_per_second': 111.582, 'eval_steps_per_second': 13.948, 'epoch': 1.92}
{'loss': 3.0878, 'learning_rate': 1.0000000000000002e-06, 'epoch': 1.96}
{'eval_loss': 3.0209360122680664, 'eval_runtime': 8.9128, 'eval_samples_per_second': 112.198, 'eval_steps_per_second': 14.025, 'epoch': 1.96}
{'loss': 3.0975, 'learning_rate': 0.0, 'epoch': 2.0}
{'eval_loss': 3.020799160003662, 'eval_runtime': 8.9407, 'eval_samples_per_second': 111.848, 'eval_steps_per_second': 13.981, 'epoch': 2.0}
{'train_runtime': 1180.7008, 'train_samples_per_second': 16.939, 'train_steps_per_second': 4.235, 'train_loss': 3.1616451721191408, 'epoch': 2.0}
Fine-tuning completed successfully.
Job Ended: Fri May 23 16:02:48 AEST 2025

+------------------- Job Report: 568312 (COMPLETED) -------------------+
| Memory (RAM)  [                    ]  4.4% (2.8 GB peak / 64 GB)     |
| CPU           [##                  ] 11.6% average                   |
| GPU           [#############       ] 67.2% average                   |
| Time          [------->            ] 35.3% (0-00:21:12 / 0-01:00:00) |
|                                                                      |
| Lustre Filesystem:                                                   |
|   Path    Total Read    Total Write    Total IOPS                    |
|   ------  ------------  -------------  ------------                  |
|   /apps   0 B           0 B            3                             |
|   /fred   506.6 MB      10.7 GB        27.9 K                        |
|   /home   8 KB          0 B            15                            |
|   OS      0 B           0 B            14                            |
|                                                                      |
| Warnings:                                                            |
|   - Too much memory requested                                        |
|   - CPU usage is low                                                 |
|   - GPU usage is low                                                 |
|   - Too much time requested                                          |
+----------------------------------------------------------------------+
